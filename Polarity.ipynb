{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Carga de paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import string\n",
    "import nltk\n",
    "import emoji\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Importar datos dentro de un dataframe de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/simonve_data.csv', sep=';', usecols=['comment_id', 'text', 'polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38768</td>\n",
       "      <td>donde lo compra a ese precio???? ..... le estan viendo la cara.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41962</td>\n",
       "      <td>David Foronda (Podemos): “A favor de regular los transgénicos, hay que tener en cuenta la soberanía alimentar... https://t.co/9dhQHZNfXn</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46156</td>\n",
       "      <td>RT @segbruce: 21% la aprueba, 73% la rechaza.  Así como va, Michelle Bachelet desaparecerá de las encuestas, ya no la apoyan ni sus votante…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52072</td>\n",
       "      <td>RT @rpl2010: #CUIDATUDINEROMV #LANUEVAMAYORÍA O #LAVIEJAPILLERÍA #AFP #INJUSTAS EDUARDO #FREI RICARDO #LAGOS Y MICHELLE #BACHELET https://t…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40533</td>\n",
       "      <td>RT @Chevige: Ricardo Lagos el mismo que utilizó la LEY ANTITERRORISTA DE PINOCHET para reprimir, aplaude a Almagro #NuevaMayoria https://t.…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  \\\n",
       "0       38768   \n",
       "1       41962   \n",
       "2       46156   \n",
       "3       52072   \n",
       "4       40533   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                                                                               donde lo compra a ese precio???? ..... le estan viendo la cara.   \n",
       "1      David Foronda (Podemos): “A favor de regular los transgénicos, hay que tener en cuenta la soberanía alimentar... https://t.co/9dhQHZNfXn   \n",
       "2  RT @segbruce: 21% la aprueba, 73% la rechaza.  Así como va, Michelle Bachelet desaparecerá de las encuestas, ya no la apoyan ni sus votante…   \n",
       "3  RT @rpl2010: #CUIDATUDINEROMV #LANUEVAMAYORÍA O #LAVIEJAPILLERÍA #AFP #INJUSTAS EDUARDO #FREI RICARDO #LAGOS Y MICHELLE #BACHELET https://t…   \n",
       "4  RT @Chevige: Ricardo Lagos el mismo que utilizó la LEY ANTITERRORISTA DE PINOCHET para reprimir, aplaude a Almagro #NuevaMayoria https://t.…   \n",
       "\n",
       "   polarity  \n",
       "0        10  \n",
       "1        -1  \n",
       "2        -1  \n",
       "3        -1  \n",
       "4        -1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43527 entries, 0 to 43526\n",
      "Data columns (total 3 columns):\n",
      "comment_id    43527 non-null int64\n",
      "text          43522 non-null object\n",
      "polarity      43527 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1020.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conteo del número de clasificaciones por comentario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34624</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34693</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34772</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34863</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35030</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "comment_id       \n",
       "34624           4\n",
       "34693           3\n",
       "34772           3\n",
       "34863           3\n",
       "35030           3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications_by_comment = df.groupby(['comment_id'])['comment_id'].agg(['count'])\n",
    "classifications_by_comment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remoción de comentarios con solo una clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda r: classifications_by_comment.loc[r['comment_id']]['count'] > 1, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Cálculo del valor de clasificación más frecuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby(['comment_id', 'text'])[['polarity']].agg(pd.Series.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['comment_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle  Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm   \n",
       "1                        RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet   \n",
       "2  Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh   \n",
       "3  RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…   \n",
       "4      Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle  Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG   \n",
       "\n",
       "  polarity  \n",
       "0        1  \n",
       "1       -1  \n",
       "2        0  \n",
       "3       -1  \n",
       "4        0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        13711\n",
       "polarity    13711\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remoción de comentarios con más de una clasificación más frecuente (moda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda r: type(r['polarity']) != pd.np.ndarray, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        12685\n",
       "polarity    12685\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>5485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "polarity      \n",
       "-1        5485\n",
       " 0        4647\n",
       " 1        2497\n",
       " 10         56"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['polarity']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remoción de los indefinidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[~(df.polarity == 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        12629\n",
       "polarity    12629\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle  Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm   \n",
       "1                        RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet   \n",
       "2  Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh   \n",
       "3  RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…   \n",
       "4      Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle  Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG   \n",
       "\n",
       "  polarity  \n",
       "0        1  \n",
       "1       -1  \n",
       "2        0  \n",
       "3       -1  \n",
       "4        0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Funciones de limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_tweet_user_mentions(s):\n",
    "    \"\"\"Remove twitter users from text.\"\"\"\n",
    "    return re.sub(r'@\\S+', ' ', s, re.UNICODE)\n",
    "\n",
    "def remove_tweet_hashtags(s):\n",
    "    \"\"\"Remove tweet hashtags.\"\"\"\n",
    "    return re.sub(r'#\\S+', ' ', s, re.UNICODE)\n",
    "\n",
    "def remove_links(s):\n",
    "    \"\"\"Remove links, urls from text.\"\"\"\n",
    "    return re.sub(r'(http|https)\\S+', ' ', s, re.UNICODE)\n",
    "\n",
    "def normalize_string(s):\n",
    "    \"\"\"# To lower case and remove accents marks\"\"\"\n",
    "    return unidecode.unidecode(s)\n",
    "\n",
    "def to_lower(s):\n",
    "    return s.lower()\n",
    "\n",
    "def remove_emojis(s):\n",
    "    \"\"\"Remove emojis (emoticons) from text.\"\"\"\n",
    "    return emoji.get_emoji_regexp().sub(u' ', s)\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    punctuation_symbols = string.punctuation\n",
    "    punctuation_symbols += '¿¡'\n",
    "    s = s.replace('\\u2026', ' ')  # triple dots\n",
    "    s = s.replace('\\u25ba', ' ')  # BLACK RIGHT-POINTING POINTER\n",
    "    s = s.replace('\\u201c', ' ')  # left double quote\n",
    "    s = s.replace('\\u201d', ' ')  # right double quote\n",
    "    return s.translate(str.maketrans(' ', ' ', punctuation_symbols))\n",
    "\n",
    "def remove_numbers(s):\n",
    "    return re.sub(r'\\d+', ' ', s)\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    stoplist = nltk.corpus.stopwords.words('spanish')\n",
    "    stoplist.remove('no')\n",
    "    stoplist.extend([\n",
    "        'rt',\n",
    "        'q',\n",
    "        'd',\n",
    "        'x'\n",
    "    ])\n",
    "    return u' '.join([w for w in s.split() if w not in stoplist])\n",
    "\n",
    "def remove_extra_whites(s):\n",
    "    \"\"\"Remove white characters repetitions from text.\"\"\"\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "def normalize(s):\n",
    "    s = remove_tweet_user_mentions(s)\n",
    "    s = remove_tweet_hashtags(s)\n",
    "    s = remove_links(s)\n",
    "    s = to_lower(s)\n",
    "    s = remove_punctuation(s)\n",
    "    s = remove_stopwords(s)\n",
    "    s = remove_emojis(s)\n",
    "    s = remove_numbers(s)\n",
    "    s = remove_extra_whites(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalized_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalized_df['text'] = normalized_df['text'].map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faltan dias acabe pesadilla llamada michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hace pocos días amigos masificaron imagen supuesto vino michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                        abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia   \n",
       "1                                            faltan dias acabe pesadilla llamada michelle bachelet   \n",
       "2               michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo   \n",
       "3                        hace pocos días amigos masificaron imagen supuesto vino michelle bachelet   \n",
       "4  alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá   \n",
       "\n",
       "  polarity  \n",
       "0        1  \n",
       "1       -1  \n",
       "2        0  \n",
       "3       -1  \n",
       "4        0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remoción de comentarios con solo una palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "normalized_df = normalized_df[normalized_df.apply(lambda r: len(r.text.split()) > 1, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        12436\n",
       "polarity    12436\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_df.text.tolist()\n",
    "y = normalized_df.polarity.astype(int).tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.610128617363344\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.87      0.71      1112\n",
      "           0       0.59      0.53      0.56       881\n",
      "           1       0.85      0.17      0.29       495\n",
      "\n",
      "    accuracy                           0.61      2488\n",
      "   macro avg       0.68      0.52      0.52      2488\n",
      "weighted avg       0.65      0.61      0.57      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6089228295819936\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.89      0.71      1112\n",
      "           0       0.63      0.41      0.50       881\n",
      "           1       0.67      0.34      0.45       495\n",
      "\n",
      "    accuracy                           0.61      2488\n",
      "   macro avg       0.63      0.55      0.55      2488\n",
      "weighted avg       0.62      0.61      0.58      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=1, penalty='l2',\n",
       "                                    random_state=None, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5727491961414791\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.66      0.66      1112\n",
      "           0       0.50      0.54      0.52       881\n",
      "           1       0.51      0.43      0.47       495\n",
      "\n",
      "    accuracy                           0.57      2488\n",
      "   macro avg       0.56      0.54      0.55      2488\n",
      "weighted avg       0.57      0.57      0.57      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - solo positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neutral_df = normalized_df[~(normalized_df.polarity == 0)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>5433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "polarity      \n",
       "-1        5433\n",
       " 1        2446"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_neutral_df.groupby(['polarity']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_neutral = no_neutral_df.text\n",
    "y_no_neutral = no_neutral_df.polarity.astype(int)\n",
    "X_train_no_neutral, X_test_no_neutral, y_train_no_neutral, y_test_no_neutral = train_test_split(X_no_neutral, y_no_neutral, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_no_neutral = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb_no_neutral.fit(X_train_no_neutral, y_train_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_neutral = nb_no_neutral.predict(X_test_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7848984771573604\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred_no_neutral, y_test_no_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.99      0.87      1106\n",
      "           1       0.93      0.30      0.45       470\n",
      "\n",
      "    accuracy                           0.78      1576\n",
      "   macro avg       0.85      0.65      0.66      1576\n",
      "weighted avg       0.82      0.78      0.74      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_no_neutral, y_pred_no_neutral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Naive Bayes - Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5928456591639871\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "])\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5928456591639871\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_df.text.values\n",
    "y = pd.get_dummies(normalized_df.polarity).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final nefasto gobierno bachelet vamos terminar comiéndonos mocos orrego'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[262, 826, 15, 3, 107, 473, 9910, 6360, 6361]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bachelet: 3\n",
      "final: 262\n",
      "orrego: 6361\n"
     ]
    }
   ],
   "source": [
    "for word in ['bachelet', 'final', 'orrego']:\n",
    "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las secuencias generadas con texts_to_sequences no poseen un largo uniforme, se utiliza pad_sequence para remediar dicho resultado mediante la adición de ceros a las secuencias hasta homogeneizar el largo de estas últimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(len(t) for t in X_train_sequences)\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_sequences = pad_sequences(X_train_sequences, padding='post', maxlen=max_sequence_length)\n",
    "X_test_padded_sequences = pad_sequences(X_test_sequences, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9948, 578)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  530,   57,  128,   50,    1,   77, 9909,   23,  222,  195,\n",
       "         94,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding Layer\n",
    "\n",
    "+ [Artículo relevante](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n",
    "+ [Documentación embedding layer](https://keras.io/layers/embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 578, 50)           1286300   \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 28900)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                289010    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 1,575,343\n",
      "Trainable params: 1,575,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=max_sequence_length))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 16s 2ms/step - loss: 0.9938 - acc: 0.4729 - val_loss: 0.9369 - val_acc: 0.5764\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.6901 - acc: 0.6922 - val_loss: 0.8909 - val_acc: 0.6121\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.2297 - acc: 0.9234 - val_loss: 1.0569 - val_acc: 0.5961\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0767 - acc: 0.9785 - val_loss: 1.3043 - val_acc: 0.6009\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0435 - acc: 0.9880 - val_loss: 1.2490 - val_acc: 0.5945\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0361 - acc: 0.9885 - val_loss: 1.4697 - val_acc: 0.5957\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 16s 2ms/step - loss: 0.0268 - acc: 0.9910 - val_loss: 1.6256 - val_acc: 0.6021\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 16s 2ms/step - loss: 0.0241 - acc: 0.9917 - val_loss: 1.5649 - val_acc: 0.5884\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 16s 2ms/step - loss: 0.0215 - acc: 0.9922 - val_loss: 1.5867 - val_acc: 0.5860\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 16s 2ms/step - loss: 0.0163 - acc: 0.9938 - val_loss: 1.7547 - val_acc: 0.5796\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9941\n",
      "Testing Accuracy:  0.5796\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Preentrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Características del embedding: \n",
    "+ #dimensions = 300\n",
    "+ #vectors = 1000653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = 'embeddings/SBW-vectors-300-min5.txt'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabra dentro del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.96480e-02,  1.13360e-02,  1.99490e-02, -8.88320e-02,\n",
       "       -2.52250e-02,  5.68440e-02,  2.54730e-02,  1.40680e-02,\n",
       "        1.63694e-01, -6.71540e-02,  1.47380e-02,  2.71340e-02,\n",
       "        6.64430e-02, -4.48460e-02, -4.49870e-02, -4.08980e-02,\n",
       "        3.03110e-02,  3.41960e-02, -4.92400e-02,  8.53700e-03,\n",
       "       -6.80910e-02, -8.79380e-02,  3.53000e-02,  1.49385e-01,\n",
       "       -1.23500e-02,  1.26130e-02,  2.93500e-02,  6.95960e-02,\n",
       "        3.91110e-02,  5.76520e-02,  6.99540e-02, -6.62170e-02,\n",
       "       -4.17840e-02,  2.86230e-02,  2.67720e-02, -6.63920e-02,\n",
       "        2.95300e-03, -1.21880e-02, -3.03630e-02,  4.02220e-02,\n",
       "        3.48580e-02,  2.74690e-02, -2.90340e-02, -4.87480e-02,\n",
       "       -3.85820e-02, -5.15530e-02, -3.35010e-02, -1.90080e-02,\n",
       "        3.04300e-03,  1.10712e-01, -2.50960e-02,  1.11082e-01,\n",
       "        3.52440e-02,  1.14207e-01,  1.01950e-02,  5.15110e-02,\n",
       "       -4.06490e-02, -1.13944e-01,  4.48730e-02,  5.20110e-02,\n",
       "        6.73600e-02,  4.90540e-02, -1.27085e-01, -3.18460e-02,\n",
       "        3.28480e-02,  4.08250e-02, -8.48730e-02,  5.98010e-02,\n",
       "       -6.74240e-02,  1.65310e-02, -8.45650e-02,  5.70240e-02,\n",
       "        8.32880e-02, -1.01360e-02, -4.85080e-02,  5.17570e-02,\n",
       "        4.66640e-02,  1.81020e-02, -5.23200e-02, -7.65000e-04,\n",
       "        5.36620e-02, -9.96700e-03,  8.28580e-02,  9.06800e-03,\n",
       "        5.45750e-02, -3.46600e-03, -2.33760e-02,  2.30690e-02,\n",
       "        8.85130e-02,  1.85040e-02, -3.95030e-02, -3.29800e-02,\n",
       "       -2.13900e-03,  1.00000e-05, -1.07627e-01,  7.69900e-03,\n",
       "        4.63510e-02, -3.06200e-03,  3.05000e-02,  1.13650e-01,\n",
       "        3.25360e-02, -9.73010e-02, -1.37340e-02,  9.83450e-02,\n",
       "        8.08980e-02, -6.41730e-02, -8.87400e-03, -1.44751e-01,\n",
       "        3.75850e-02,  1.32900e-02,  5.96740e-02,  6.16300e-03,\n",
       "        7.31800e-03,  5.30000e-05, -6.02920e-02, -5.91350e-02,\n",
       "        4.94970e-02, -1.14380e-02, -9.51080e-02, -4.34650e-02,\n",
       "        4.85670e-02, -4.39900e-02, -3.07740e-02,  5.09200e-03,\n",
       "       -3.22650e-02,  9.39200e-03,  1.85030e-02,  8.48570e-02,\n",
       "        1.09709e-01, -2.06620e-02,  1.76960e-02,  2.66990e-02,\n",
       "       -7.66380e-02, -1.41060e-02, -3.51550e-02,  4.69990e-02,\n",
       "       -3.72700e-03, -4.78050e-02,  4.42700e-02,  1.13140e-02,\n",
       "        3.65240e-02, -6.95050e-02, -1.48500e-02, -3.53800e-03,\n",
       "       -4.70490e-02,  2.93490e-02,  3.45210e-02, -3.21990e-02,\n",
       "        1.16497e-01, -7.76100e-02,  6.82340e-02, -1.61260e-02,\n",
       "       -6.64540e-02, -7.99140e-02, -2.07230e-02, -6.49050e-02,\n",
       "        6.95600e-02,  2.13680e-02, -4.94970e-02, -4.65990e-02,\n",
       "        6.76630e-02, -6.90350e-02,  1.18015e-01,  2.74630e-02,\n",
       "       -6.17600e-03, -3.45140e-02, -2.65150e-02,  4.03080e-02,\n",
       "        9.11130e-02, -8.05390e-02,  1.32408e-01, -7.09580e-02,\n",
       "        1.97300e-02,  3.33990e-02,  3.48900e-03, -1.59659e-01,\n",
       "       -4.23000e-03,  4.88800e-03, -5.66150e-02,  6.10210e-02,\n",
       "        2.51170e-02,  9.96130e-02,  6.38760e-02, -6.20200e-03,\n",
       "       -4.93160e-02, -2.05300e-02,  8.52200e-03, -9.41680e-02,\n",
       "       -9.45100e-03, -3.46820e-02, -2.68010e-02,  6.53830e-02,\n",
       "        4.25280e-02, -1.36880e-02,  3.50680e-02,  1.19803e-01,\n",
       "       -2.02780e-02,  1.11882e-01, -6.83360e-02,  5.02450e-02,\n",
       "       -1.23240e-01,  2.24800e-03,  1.02290e-02, -6.25400e-02,\n",
       "       -1.78370e-02, -1.15210e-01,  5.10360e-02,  2.69200e-02,\n",
       "        8.34570e-02,  6.29020e-02,  3.08300e-03, -3.71120e-02,\n",
       "       -1.54250e-02,  1.71600e-03,  1.80020e-02,  1.01100e-01,\n",
       "        1.94460e-02,  7.48390e-02,  5.99510e-02,  7.22430e-02,\n",
       "       -2.54590e-02, -3.98530e-02,  7.79500e-02,  6.01990e-02,\n",
       "       -7.06030e-02,  6.06520e-02,  2.38550e-02, -3.58580e-02,\n",
       "       -5.80430e-02, -7.51300e-02,  6.70000e-04,  8.28280e-02,\n",
       "       -1.31410e-02,  1.44692e-01, -1.05775e-01,  8.76240e-02,\n",
       "        4.70300e-02, -1.57000e-02, -4.23600e-02,  2.63370e-02,\n",
       "        1.44966e-01,  2.19220e-02,  1.20970e-02, -1.14430e-02,\n",
       "       -1.10208e-01,  9.33300e-03, -8.14230e-02, -2.71370e-02,\n",
       "        8.27000e-04,  8.53890e-02,  3.44640e-02,  3.23380e-02,\n",
       "       -1.50400e-02,  9.95400e-03, -1.17590e-02, -4.23950e-02,\n",
       "       -4.67000e-04,  5.50470e-02, -4.97100e-02, -1.04978e-01,\n",
       "        3.17420e-02,  3.70200e-02, -7.01600e-03, -4.97120e-02,\n",
       "        4.16840e-02,  1.83480e-02, -1.20100e-03,  1.92820e-02,\n",
       "        5.76300e-03,  7.49520e-02, -1.85880e-02,  1.60550e-02,\n",
       "        1.19526e-01, -1.46130e-02,  2.05980e-02,  2.73120e-02,\n",
       "        2.42520e-02, -2.40440e-02, -2.63320e-02, -6.31520e-02,\n",
       "       -9.53630e-02, -3.43350e-02, -6.25520e-02, -3.57300e-02,\n",
       "        9.11650e-02,  9.68600e-03,  2.03410e-02, -1.20040e-02,\n",
       "        1.18260e-02, -8.43010e-02,  1.11440e-02,  7.49690e-02,\n",
       "       -4.86200e-03, -1.40760e-02,  2.56920e-02, -7.73220e-02,\n",
       "       -2.29980e-02, -1.28057e-01, -4.91700e-03,  6.26280e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors['de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### palabra fuera del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'bachelet' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-49ef493d85b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bachelet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'bachelet' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "wordvectors['bachelet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25726, 300)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los vectores para el vocabulario del corpus de entrenamiento, desde el modelo word2vect preentrenado. Si no se encuentra el vector para alguna palabra (Out of Vocabulary Word), se genera uno aleatorio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = wordvectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25726, 300)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 173400)            0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 10)                1734010   \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 9,451,843\n",
      "Trainable params: 9,451,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "                    input_dim=vocab_size, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=max_sequence_length,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True\n",
    "                )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 82s 8ms/step - loss: 0.9670 - acc: 0.5251 - val_loss: 0.8776 - val_acc: 0.5687\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.4567 - acc: 0.8298 - val_loss: 0.9078 - val_acc: 0.5997\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 84s 8ms/step - loss: 0.1077 - acc: 0.9698 - val_loss: 1.0890 - val_acc: 0.5824\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 81s 8ms/step - loss: 0.0520 - acc: 0.9863 - val_loss: 1.4341 - val_acc: 0.5273\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0371 - acc: 0.9898 - val_loss: 1.3180 - val_acc: 0.5868\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 84s 8ms/step - loss: 0.0325 - acc: 0.9918 - val_loss: 1.4816 - val_acc: 0.5623\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0242 - acc: 0.9915 - val_loss: 1.5348 - val_acc: 0.5957\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0207 - acc: 0.9920 - val_loss: 1.6459 - val_acc: 0.5896\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0172 - acc: 0.9929 - val_loss: 1.6186 - val_acc: 0.5844\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0273 - acc: 0.9902 - val_loss: 2.0997 - val_acc: 0.5744\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9943\n",
      "Testing Accuracy:  0.5744\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado de michelle bachelet otorgó asesoría jurídica a mujer que realizó la denuncia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rt faltan dias para que se acabe esta pesadilla llamada michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle bachelet está trotando para estar en forma michelle bachelet está tratando de aprobar sus reformas chavo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt hace pocos días los amigos de masificaron imagen sobre supuesto vino de michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcalde de pozo almonte josé fernando muñoz junto a la presidenta michelle bachelet e intendenta de tarapacá</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                text  \\\n",
       "0                              abogado de michelle bachelet otorgó asesoría jurídica a mujer que realizó la denuncia   \n",
       "1                                          rt faltan dias para que se acabe esta pesadilla llamada michelle bachelet   \n",
       "2  michelle bachelet está trotando para estar en forma michelle bachelet está tratando de aprobar sus reformas chavo   \n",
       "3                       rt hace pocos días los amigos de masificaron imagen sobre supuesto vino de michelle bachelet   \n",
       "4       alcalde de pozo almonte josé fernando muñoz junto a la presidenta michelle bachelet e intendenta de tarapacá   \n",
       "\n",
       "  polarity  \n",
       "0        1  \n",
       "1       -1  \n",
       "2        0  \n",
       "3       -1  \n",
       "4        0  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_sw_df = df.copy()\n",
    "def normalize_with_sw(s):\n",
    "    s = remove_tweet_user_mentions(s)\n",
    "    s = remove_tweet_hashtags(s)\n",
    "    s = remove_links(s)\n",
    "    s = to_lower(s)\n",
    "    s = remove_punctuation(s)\n",
    "    s = remove_emojis(s)\n",
    "    s = remove_numbers(s)\n",
    "    s = remove_extra_whites(s)\n",
    "    return s\n",
    "    \n",
    "normalized_sw_df['text'] = normalized_sw_df['text'].map(normalize_with_sw)\n",
    "normalized_sw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sw = normalized_sw_df.text.values\n",
    "y_sw = pd.get_dummies(normalized_sw_df.polarity).values\n",
    "X_sw_train, X_sw_test, y_sw_train, y_sw_test = train_test_split(X_sw, y_sw, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_sw = Tokenizer()\n",
    "tokenizer_sw.fit_on_texts(X_sw_train)\n",
    "X_sw_train_sequences = tokenizer_sw.texts_to_sequences(X_sw_train)\n",
    "X_sw_test_sequences = tokenizer_sw.texts_to_sequences(X_sw_test)\n",
    "vocab_size_sw = len(tokenizer_sw.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "max_sequence_length_sw = max(len(t) for t in X_sw_train_sequences)\n",
    "X_sw_train_padded_sequences = pad_sequences(X_sw_train_sequences, padding='post', maxlen=max_sequence_length_sw)\n",
    "X_sw_test_padded_sequences = pad_sequences(X_sw_test_sequences, padding='post', maxlen=max_sequence_length_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix_sw = np.zeros((vocab_size_sw, EMBEDDING_DIM))\n",
    "embedding_matrix_sw.shape\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size_sw:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = wordvectors[word]\n",
    "        embedding_matrix_sw[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix_sw[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 400, 300)          7784400   \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 120000)            0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                1200010   \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 8,984,443\n",
      "Trainable params: 1,200,043\n",
      "Non-trainable params: 7,784,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "                    input_dim=vocab_size_sw, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=max_sequence_length_sw,\n",
    "                    weights=[embedding_matrix_sw],\n",
    "                    trainable=False\n",
    "                )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10103 samples, validate on 2526 samples\n",
      "Epoch 1/10\n",
      "10103/10103 [==============================] - 15s 1ms/step - loss: 1.0191 - acc: 0.4918 - val_loss: 1.0035 - val_acc: 0.5182\n",
      "Epoch 2/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.8847 - acc: 0.5777 - val_loss: 1.0276 - val_acc: 0.5119\n",
      "Epoch 3/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.7782 - acc: 0.6379 - val_loss: 1.0818 - val_acc: 0.5055\n",
      "Epoch 4/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.6880 - acc: 0.6872 - val_loss: 1.1193 - val_acc: 0.5071\n",
      "Epoch 5/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.6160 - acc: 0.7342 - val_loss: 1.2145 - val_acc: 0.4972\n",
      "Epoch 6/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.5497 - acc: 0.7656 - val_loss: 1.3343 - val_acc: 0.4715\n",
      "Epoch 7/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.5034 - acc: 0.7872 - val_loss: 1.3498 - val_acc: 0.4774\n",
      "Epoch 8/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.4547 - acc: 0.8116 - val_loss: 1.4681 - val_acc: 0.4802\n",
      "Epoch 9/10\n",
      "10103/10103 [==============================] - 14s 1ms/step - loss: 0.4201 - acc: 0.8279 - val_loss: 1.5811 - val_acc: 0.4893\n",
      "Epoch 10/10\n",
      "10103/10103 [==============================] - 15s 1ms/step - loss: 0.3876 - acc: 0.8436 - val_loss: 1.6837 - val_acc: 0.4770\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_sw_train_padded_sequences, y_sw_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_sw_test_padded_sequences, y_sw_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "[model reference](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "288.85px",
    "left": "205px",
    "right": "361px",
    "top": "177px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
