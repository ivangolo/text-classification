{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/emotion.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Happiness: 0\n",
    "+ Sadness: 1\n",
    "+ Fear: 2\n",
    "+ Anger: 3\n",
    "+ Surprise: 4\n",
    "+ Disgust: 5\n",
    "+ Undefined: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       11342\n",
       "emotion    11342\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text\n",
       "emotion      \n",
       "0        2557\n",
       "1         356\n",
       "2         147\n",
       "3        1338\n",
       "4        3253\n",
       "5        3420\n",
       "10        271"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['emotion']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       957\n",
       "emotion    957\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.str.contains('bachelet', case=False)].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faltan dias acabe pesadilla llamada michelle bachelet</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presidenta michelle bachelet promulga ley equidad tarifaria servicios eléctricos</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>serio nombre completo michelle bachelet queen</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                        abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia   \n",
       "1                                            faltan dias acabe pesadilla llamada michelle bachelet   \n",
       "2  alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá   \n",
       "3                 presidenta michelle bachelet promulga ley equidad tarifaria servicios eléctricos   \n",
       "4                                                    serio nombre completo michelle bachelet queen   \n",
       "\n",
       "   emotion  \n",
       "0        4  \n",
       "1        5  \n",
       "2        4  \n",
       "3        0  \n",
       "4        4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].map(text_preprocessing.normalize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove texts with only one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       11156\n",
       "emotion    11156\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.apply(lambda r: len(r.text.split()) > 1, axis=1)]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       10040\n",
       "emotion    10040\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text       1116\n",
       "emotion    1116\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text.values\n",
    "y_train = train.emotion.values\n",
    "X_test = test.text.values\n",
    "y_test = test.emotion.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(y_train)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6366115 ,  4.61185117, 11.11849391,  1.20124432,  0.49853518,\n",
       "        0.47133937,  6.15573268])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.532258064516129\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.48      0.57       244\n",
      "           1       0.00      0.00      0.00        35\n",
      "           2       0.00      0.00      0.00        17\n",
      "           3       0.00      0.00      0.00       131\n",
      "           4       0.55      0.65      0.60       321\n",
      "           5       0.47      0.78      0.59       344\n",
      "          10       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.53      1116\n",
      "   macro avg       0.25      0.27      0.25      1116\n",
      "weighted avg       0.46      0.53      0.48      1116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SVC(class_weight='balanced')),\n",
    "               ])\n",
    "svm_rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.532258064516129\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.48      0.57       244\n",
      "           1       0.00      0.00      0.00        35\n",
      "           2       0.00      0.00      0.00        17\n",
      "           3       0.00      0.00      0.00       131\n",
      "           4       0.55      0.65      0.60       321\n",
      "           5       0.47      0.78      0.59       344\n",
      "          10       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.53      1116\n",
      "   macro avg       0.25      0.27      0.25      1116\n",
      "weighted avg       0.46      0.53      0.48      1116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "CHECKPOINTS_PATH = '../model_checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train.emotion).values\n",
    "y_test = pd.get_dummies(test.emotion).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "max_sequence_length = max(len(t) for t in X_train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25852"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_sequences = pad_sequences(X_train_sequences, padding='post', maxlen=max_sequence_length)\n",
    "X_test_padded_sequences = pad_sequences(X_test_sequences, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = KeyedVectors.load_word2vec_format('../embeddings/glove-sbwc.i25.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = glove_vectors[word]\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        glove_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 578, 300)          7755600   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 574, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 114, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 110, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 18, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 8,129,239\n",
      "Trainable params: 8,129,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length, \n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=5),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=5),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10040 samples, validate on 1116 samples\n",
      "Epoch 1/10\n",
      "10040/10040 [==============================] - 138s 14ms/step - loss: 0.0816 - acc: 0.9735 - val_loss: 2.8943 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50000, saving model to ../model_checkpoints/cnn-yk-best.hdf5\n",
      "Epoch 2/10\n",
      "10040/10040 [==============================] - 139s 14ms/step - loss: 0.0593 - acc: 0.9788 - val_loss: 2.7365 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50000 to 0.50448, saving model to ../model_checkpoints/cnn-yk-best.hdf5\n",
      "Epoch 3/10\n",
      "10040/10040 [==============================] - 134s 13ms/step - loss: 0.0436 - acc: 0.9847 - val_loss: 3.2995 - val_acc: 0.5090\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.50448 to 0.50896, saving model to ../model_checkpoints/cnn-yk-best.hdf5\n",
      "Epoch 4/10\n",
      "10040/10040 [==============================] - 134s 13ms/step - loss: 0.0282 - acc: 0.9892 - val_loss: 3.0596 - val_acc: 0.4928\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.50896\n",
      "Epoch 5/10\n",
      "10040/10040 [==============================] - 134s 13ms/step - loss: 0.0205 - acc: 0.9895 - val_loss: 3.2116 - val_acc: 0.4668\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.50896\n",
      "Epoch 6/10\n",
      "10040/10040 [==============================] - 134s 13ms/step - loss: 0.0160 - acc: 0.9920 - val_loss: 3.4834 - val_acc: 0.4884\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50896\n",
      "Epoch 7/10\n",
      "10040/10040 [==============================] - 134s 13ms/step - loss: 0.0175 - acc: 0.9906 - val_loss: 3.6280 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50896\n",
      "Epoch 8/10\n",
      "10040/10040 [==============================] - 135s 13ms/step - loss: 0.0251 - acc: 0.9886 - val_loss: 3.6492 - val_acc: 0.5099\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.50896 to 0.50986, saving model to ../model_checkpoints/cnn-yk-best.hdf5\n",
      "Epoch 9/10\n",
      "10040/10040 [==============================] - 188s 19ms/step - loss: 0.0600 - acc: 0.9806 - val_loss: 3.7391 - val_acc: 0.4857\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.50986\n",
      "Epoch 10/10\n",
      "10040/10040 [==============================] - 150s 15ms/step - loss: 0.0691 - acc: 0.9751 - val_loss: 3.1873 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.50986\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-yk'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=10,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGru + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 578, 300)          7755600   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 7,896,663\n",
      "Trainable params: 7,896,663\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10040 samples, validate on 1116 samples\n",
      "Epoch 1/10\n",
      "10040/10040 [==============================] - 251s 25ms/step - loss: 0.0649 - acc: 0.9821 - val_loss: 2.0699 - val_acc: 0.4982\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49821, saving model to ../model_checkpoints/bigru-2019-05-30T13:33:50.624456.hdf5\n",
      "Epoch 2/10\n",
      "10040/10040 [==============================] - 227s 23ms/step - loss: 0.0467 - acc: 0.9879 - val_loss: 2.2517 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.49821 to 0.50179, saving model to ../model_checkpoints/bigru-2019-05-30T13:33:50.624456.hdf5\n",
      "Epoch 3/10\n",
      "10040/10040 [==============================] - 227s 23ms/step - loss: 0.0388 - acc: 0.9886 - val_loss: 2.3076 - val_acc: 0.5054\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.50179 to 0.50538, saving model to ../model_checkpoints/bigru-2019-05-30T13:33:50.624456.hdf5\n",
      "Epoch 4/10\n",
      "10040/10040 [==============================] - 220s 22ms/step - loss: 0.0351 - acc: 0.9894 - val_loss: 2.3828 - val_acc: 0.5054\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.50538 to 0.50538, saving model to ../model_checkpoints/bigru-2019-05-30T13:33:50.624456.hdf5\n",
      "Epoch 5/10\n",
      "10040/10040 [==============================] - 220s 22ms/step - loss: 0.0339 - acc: 0.9890 - val_loss: 2.4069 - val_acc: 0.5090\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.50538 to 0.50896, saving model to ../model_checkpoints/bigru-2019-05-30T13:33:50.624456.hdf5\n",
      "Epoch 6/10\n",
      "10040/10040 [==============================] - 227s 23ms/step - loss: 0.0294 - acc: 0.9893 - val_loss: 2.4488 - val_acc: 0.5027\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.50896\n",
      "Epoch 7/10\n",
      "10040/10040 [==============================] - 227s 23ms/step - loss: 0.0293 - acc: 0.9889 - val_loss: 2.5599 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.50896\n",
      "Epoch 8/10\n",
      "10040/10040 [==============================] - 226s 22ms/step - loss: 0.0268 - acc: 0.9897 - val_loss: 2.5222 - val_acc: 0.4973\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.50896\n",
      "Epoch 9/10\n",
      "10040/10040 [==============================] - 229s 23ms/step - loss: 0.0250 - acc: 0.9907 - val_loss: 2.6910 - val_acc: 0.4857\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.50896\n",
      "Epoch 10/10\n",
      "10040/10040 [==============================] - 241s 24ms/step - loss: 0.0241 - acc: 0.9902 - val_loss: 2.7060 - val_acc: 0.5099\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.50896 to 0.50986, saving model to ../model_checkpoints/bigru-2019-05-30T13:33:50.624456.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=10,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + CNN + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 578, 300)          7755600   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 574, 64)           41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 7,941,399\n",
      "Trainable params: 7,941,399\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10040 samples, validate on 1116 samples\n",
      "Epoch 1/10\n",
      "10040/10040 [==============================] - 260s 26ms/step - loss: 1.3345 - acc: 0.4944 - val_loss: 1.2023 - val_acc: 0.5430\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.54301, saving model to ../model_checkpoints/bigru-cnn-2019-05-30T15:26:29.718466.hdf5\n",
      "Epoch 2/10\n",
      "10040/10040 [==============================] - 258s 26ms/step - loss: 0.9828 - acc: 0.6387 - val_loss: 1.2133 - val_acc: 0.5529\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.54301 to 0.55287, saving model to ../model_checkpoints/bigru-cnn-2019-05-30T15:26:29.718466.hdf5\n",
      "Epoch 3/10\n",
      "10040/10040 [==============================] - 256s 26ms/step - loss: 0.5327 - acc: 0.8173 - val_loss: 1.5088 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.55287\n",
      "Epoch 4/10\n",
      "10040/10040 [==============================] - 254s 25ms/step - loss: 0.2099 - acc: 0.9351 - val_loss: 1.8738 - val_acc: 0.5206\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.55287\n",
      "Epoch 5/10\n",
      "10040/10040 [==============================] - 249s 25ms/step - loss: 0.1019 - acc: 0.9710 - val_loss: 2.2353 - val_acc: 0.5045\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.55287\n",
      "Epoch 6/10\n",
      "10040/10040 [==============================] - 246s 25ms/step - loss: 0.0653 - acc: 0.9818 - val_loss: 2.3664 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.55287\n",
      "Epoch 7/10\n",
      "10040/10040 [==============================] - 247s 25ms/step - loss: 0.0488 - acc: 0.9867 - val_loss: 2.5289 - val_acc: 0.5027\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.55287\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-cnn'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=10,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
