{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/polarity-extended.csv', sep=\";\")\n",
    "df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm   \n",
       "1                        RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet   \n",
       "2  Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh   \n",
       "3  RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…   \n",
       "4       Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        13691\n",
       "polarity    13691\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>5903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "polarity      \n",
       "-1        5903\n",
       " 0        4778\n",
       " 1        2815\n",
       " 10        195"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['polarity']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(text_preprocessing.normalize, no_tweet_hashtags=True, no_camel_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faltan dias acabe pesadilla llamada michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hace pocos días amigos masificaron imagen supuesto vino michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                        abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia   \n",
       "1                                            faltan dias acabe pesadilla llamada michelle bachelet   \n",
       "2               michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo   \n",
       "3                        hace pocos días amigos masificaron imagen supuesto vino michelle bachelet   \n",
       "4  alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove texts with only one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda r: len(r.text.split()) > 1, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        13475\n",
       "polarity    13475\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        12127\n",
       "polarity    12127\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        1348\n",
       "polarity    1348\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text.values\n",
    "y_train = train.polarity.values\n",
    "X_test = test.text.values\n",
    "y_test = test.polarity.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6016320474777448\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.58      0.90      0.71       609\n",
      "           0       0.61      0.45      0.51       453\n",
      "           1       0.78      0.23      0.36       265\n",
      "          10       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.60      1348\n",
      "   macro avg       0.49      0.39      0.40      1348\n",
      "weighted avg       0.62      0.60      0.56      1348\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6023738872403561\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.89      0.71       609\n",
      "           0       0.65      0.38      0.47       453\n",
      "           1       0.61      0.37      0.46       265\n",
      "          10       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.60      1348\n",
      "   macro avg       0.46      0.41      0.41      1348\n",
      "weighted avg       0.60      0.60      0.57      1348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=1, penalty='l2',\n",
       "                                    random_state=None, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5719584569732937\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.66      0.67       609\n",
      "           0       0.50      0.53      0.52       453\n",
      "           1       0.50      0.48      0.49       265\n",
      "          10       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.57      1348\n",
      "   macro avg       0.42      0.42      0.42      1348\n",
      "weighted avg       0.57      0.57      0.57      1348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - solo positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neutral_df = df[~(df.polarity == 0)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>5846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "polarity      \n",
       "-1        5846\n",
       " 1        2758\n",
       " 10        185"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_neutral_df.groupby(['polarity']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_neutral, test_no_neutral = train_test_split(no_neutral_df, test_size=0.2, random_state=42)\n",
    "X_train_no_neutral = train_no_neutral.text.values\n",
    "X_test_no_neutral = test_no_neutral.text.values\n",
    "y_train_no_neutral = train_no_neutral.polarity.values\n",
    "y_test_no_neutral = test_no_neutral.polarity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_no_neutral = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb_no_neutral.fit(X_train_no_neutral, y_train_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_neutral = nb_no_neutral.predict(X_test_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.754835039817975\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred_no_neutral, y_test_no_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.98      0.84      1180\n",
      "           1       0.89      0.30      0.45       544\n",
      "          10       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.75      1758\n",
      "   macro avg       0.54      0.43      0.43      1758\n",
      "weighted avg       0.77      0.75      0.71      1758\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_no_neutral, y_pred_no_neutral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5838278931750742\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "CHECKPOINTS_PATH = '../model_checkpoints/polarity/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train.polarity).values\n",
    "y_test = pd.get_dummies(test.polarity).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ahora mismo dicen oye lista clinton atenta libertad expresión'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 40, 149, 1160, 457, 28, 7252, 371, 2607]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bachelet: 4\n",
      "final: 218\n",
      "orrego: 9228\n"
     ]
    }
   ],
   "source": [
    "for word in ['bachelet', 'final', 'orrego']:\n",
    "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las secuencias generadas con texts_to_sequences no poseen un largo uniforme, se utiliza pad_sequence para remediar dicho resultado mediante la adición de ceros a las secuencias hasta homogeneizar el largo de estas últimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(len(t) for t in X_train_sequences)\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_sequences = pad_sequences(X_train_sequences, padding='post', maxlen=max_sequence_length)\n",
    "X_test_padded_sequences = pad_sequences(X_test_sequences, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12127, 578)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 295,  524, 1414, 1237, 2606,  546, 3623,  444, 1849, 1238,    3,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding Layer\n",
    "\n",
    "+ [Artículo relevante](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n",
    "+ [Documentación embedding layer](https://keras.io/layers/embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 578, 50)           1450700   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28900)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1849664   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 3,300,624\n",
      "Trainable params: 3,300,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=max_sequence_length))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 14s 1ms/step - loss: 0.0302 - acc: 0.9904 - val_loss: 1.6759 - val_acc: 0.5972\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59718, saving model to ../model_checkpoints/polarity/ffn-2019-06-04T11:24:47.043451.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 13s 1ms/step - loss: 0.0234 - acc: 0.9914 - val_loss: 1.5773 - val_acc: 0.6024\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59718 to 0.60237, saving model to ../model_checkpoints/polarity/ffn-2019-06-04T11:24:47.043451.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 14s 1ms/step - loss: 0.0207 - acc: 0.9921 - val_loss: 1.6549 - val_acc: 0.6046\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.60237 to 0.60460, saving model to ../model_checkpoints/polarity/ffn-2019-06-04T11:24:47.043451.hdf5\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 14s 1ms/step - loss: 0.0185 - acc: 0.9923 - val_loss: 1.6445 - val_acc: 0.5979\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60460\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ffn'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9938\n",
      "Testing Accuracy:  0.5979\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Preentrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Características del embedding: \n",
    "+ #dimensions = 300\n",
    "+ #vectors = 1000653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = '../embeddings/SBW-vectors-300-min5.txt'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabra dentro del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.96480e-02,  1.13360e-02,  1.99490e-02, -8.88320e-02,\n",
       "       -2.52250e-02,  5.68440e-02,  2.54730e-02,  1.40680e-02,\n",
       "        1.63694e-01, -6.71540e-02,  1.47380e-02,  2.71340e-02,\n",
       "        6.64430e-02, -4.48460e-02, -4.49870e-02, -4.08980e-02,\n",
       "        3.03110e-02,  3.41960e-02, -4.92400e-02,  8.53700e-03,\n",
       "       -6.80910e-02, -8.79380e-02,  3.53000e-02,  1.49385e-01,\n",
       "       -1.23500e-02,  1.26130e-02,  2.93500e-02,  6.95960e-02,\n",
       "        3.91110e-02,  5.76520e-02,  6.99540e-02, -6.62170e-02,\n",
       "       -4.17840e-02,  2.86230e-02,  2.67720e-02, -6.63920e-02,\n",
       "        2.95300e-03, -1.21880e-02, -3.03630e-02,  4.02220e-02,\n",
       "        3.48580e-02,  2.74690e-02, -2.90340e-02, -4.87480e-02,\n",
       "       -3.85820e-02, -5.15530e-02, -3.35010e-02, -1.90080e-02,\n",
       "        3.04300e-03,  1.10712e-01, -2.50960e-02,  1.11082e-01,\n",
       "        3.52440e-02,  1.14207e-01,  1.01950e-02,  5.15110e-02,\n",
       "       -4.06490e-02, -1.13944e-01,  4.48730e-02,  5.20110e-02,\n",
       "        6.73600e-02,  4.90540e-02, -1.27085e-01, -3.18460e-02,\n",
       "        3.28480e-02,  4.08250e-02, -8.48730e-02,  5.98010e-02,\n",
       "       -6.74240e-02,  1.65310e-02, -8.45650e-02,  5.70240e-02,\n",
       "        8.32880e-02, -1.01360e-02, -4.85080e-02,  5.17570e-02,\n",
       "        4.66640e-02,  1.81020e-02, -5.23200e-02, -7.65000e-04,\n",
       "        5.36620e-02, -9.96700e-03,  8.28580e-02,  9.06800e-03,\n",
       "        5.45750e-02, -3.46600e-03, -2.33760e-02,  2.30690e-02,\n",
       "        8.85130e-02,  1.85040e-02, -3.95030e-02, -3.29800e-02,\n",
       "       -2.13900e-03,  1.00000e-05, -1.07627e-01,  7.69900e-03,\n",
       "        4.63510e-02, -3.06200e-03,  3.05000e-02,  1.13650e-01,\n",
       "        3.25360e-02, -9.73010e-02, -1.37340e-02,  9.83450e-02,\n",
       "        8.08980e-02, -6.41730e-02, -8.87400e-03, -1.44751e-01,\n",
       "        3.75850e-02,  1.32900e-02,  5.96740e-02,  6.16300e-03,\n",
       "        7.31800e-03,  5.30000e-05, -6.02920e-02, -5.91350e-02,\n",
       "        4.94970e-02, -1.14380e-02, -9.51080e-02, -4.34650e-02,\n",
       "        4.85670e-02, -4.39900e-02, -3.07740e-02,  5.09200e-03,\n",
       "       -3.22650e-02,  9.39200e-03,  1.85030e-02,  8.48570e-02,\n",
       "        1.09709e-01, -2.06620e-02,  1.76960e-02,  2.66990e-02,\n",
       "       -7.66380e-02, -1.41060e-02, -3.51550e-02,  4.69990e-02,\n",
       "       -3.72700e-03, -4.78050e-02,  4.42700e-02,  1.13140e-02,\n",
       "        3.65240e-02, -6.95050e-02, -1.48500e-02, -3.53800e-03,\n",
       "       -4.70490e-02,  2.93490e-02,  3.45210e-02, -3.21990e-02,\n",
       "        1.16497e-01, -7.76100e-02,  6.82340e-02, -1.61260e-02,\n",
       "       -6.64540e-02, -7.99140e-02, -2.07230e-02, -6.49050e-02,\n",
       "        6.95600e-02,  2.13680e-02, -4.94970e-02, -4.65990e-02,\n",
       "        6.76630e-02, -6.90350e-02,  1.18015e-01,  2.74630e-02,\n",
       "       -6.17600e-03, -3.45140e-02, -2.65150e-02,  4.03080e-02,\n",
       "        9.11130e-02, -8.05390e-02,  1.32408e-01, -7.09580e-02,\n",
       "        1.97300e-02,  3.33990e-02,  3.48900e-03, -1.59659e-01,\n",
       "       -4.23000e-03,  4.88800e-03, -5.66150e-02,  6.10210e-02,\n",
       "        2.51170e-02,  9.96130e-02,  6.38760e-02, -6.20200e-03,\n",
       "       -4.93160e-02, -2.05300e-02,  8.52200e-03, -9.41680e-02,\n",
       "       -9.45100e-03, -3.46820e-02, -2.68010e-02,  6.53830e-02,\n",
       "        4.25280e-02, -1.36880e-02,  3.50680e-02,  1.19803e-01,\n",
       "       -2.02780e-02,  1.11882e-01, -6.83360e-02,  5.02450e-02,\n",
       "       -1.23240e-01,  2.24800e-03,  1.02290e-02, -6.25400e-02,\n",
       "       -1.78370e-02, -1.15210e-01,  5.10360e-02,  2.69200e-02,\n",
       "        8.34570e-02,  6.29020e-02,  3.08300e-03, -3.71120e-02,\n",
       "       -1.54250e-02,  1.71600e-03,  1.80020e-02,  1.01100e-01,\n",
       "        1.94460e-02,  7.48390e-02,  5.99510e-02,  7.22430e-02,\n",
       "       -2.54590e-02, -3.98530e-02,  7.79500e-02,  6.01990e-02,\n",
       "       -7.06030e-02,  6.06520e-02,  2.38550e-02, -3.58580e-02,\n",
       "       -5.80430e-02, -7.51300e-02,  6.70000e-04,  8.28280e-02,\n",
       "       -1.31410e-02,  1.44692e-01, -1.05775e-01,  8.76240e-02,\n",
       "        4.70300e-02, -1.57000e-02, -4.23600e-02,  2.63370e-02,\n",
       "        1.44966e-01,  2.19220e-02,  1.20970e-02, -1.14430e-02,\n",
       "       -1.10208e-01,  9.33300e-03, -8.14230e-02, -2.71370e-02,\n",
       "        8.27000e-04,  8.53890e-02,  3.44640e-02,  3.23380e-02,\n",
       "       -1.50400e-02,  9.95400e-03, -1.17590e-02, -4.23950e-02,\n",
       "       -4.67000e-04,  5.50470e-02, -4.97100e-02, -1.04978e-01,\n",
       "        3.17420e-02,  3.70200e-02, -7.01600e-03, -4.97120e-02,\n",
       "        4.16840e-02,  1.83480e-02, -1.20100e-03,  1.92820e-02,\n",
       "        5.76300e-03,  7.49520e-02, -1.85880e-02,  1.60550e-02,\n",
       "        1.19526e-01, -1.46130e-02,  2.05980e-02,  2.73120e-02,\n",
       "        2.42520e-02, -2.40440e-02, -2.63320e-02, -6.31520e-02,\n",
       "       -9.53630e-02, -3.43350e-02, -6.25520e-02, -3.57300e-02,\n",
       "        9.11650e-02,  9.68600e-03,  2.03410e-02, -1.20040e-02,\n",
       "        1.18260e-02, -8.43010e-02,  1.11440e-02,  7.49690e-02,\n",
       "       -4.86200e-03, -1.40760e-02,  2.56920e-02, -7.73220e-02,\n",
       "       -2.29980e-02, -1.28057e-01, -4.91700e-03,  6.26280e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors['de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29014, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los vectores para el vocabulario del corpus de entrenamiento, desde el modelo word2vect preentrenado. Si no se encuentra el vector para alguna palabra (Out of Vocabulary Word), se genera uno aleatorio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = wordvectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29014, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN + word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 173400)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                11097664  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 19,802,124\n",
      "Trainable params: 19,802,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "                    input_dim=vocab_size, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=max_sequence_length,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True\n",
    "                )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 1.0203 - acc: 0.5294 - val_loss: 0.8968 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61499, saving model to ../model_checkpoints/polarity/ffn-w2v-2019-06-04T11:33:18.613130.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 0.4494 - acc: 0.8373 - val_loss: 0.9868 - val_acc: 0.6105\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.61499\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 0.1099 - acc: 0.9679 - val_loss: 1.2182 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61499\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 84s 7ms/step - loss: 0.0543 - acc: 0.9859 - val_loss: 1.2905 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61499\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 0.0387 - acc: 0.9897 - val_loss: 1.4088 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61499\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ffn-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded_sequences)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - Yoon Kim Model + word2vec\n",
    "\n",
    "[model reference](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 574, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 191, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 187, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 33, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 9,077,452\n",
      "Trainable params: 9,077,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=5),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 1.0058 - acc: 0.5385 - val_loss: 0.9255 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59941, saving model to ../model_checkpoints/polarity/cnn-w2v-2019-06-04T11:42:59.698089.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 178s 15ms/step - loss: 0.6332 - acc: 0.7536 - val_loss: 0.9545 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59941 to 0.61721, saving model to ../model_checkpoints/polarity/cnn-w2v-2019-06-04T11:42:59.698089.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 178s 15ms/step - loss: 0.2576 - acc: 0.9130 - val_loss: 1.4142 - val_acc: 0.5638\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61721\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 0.1268 - acc: 0.9594 - val_loss: 1.5104 - val_acc: 0.6142\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61721\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 0.0644 - acc: 0.9800 - val_loss: 1.8126 - val_acc: 0.5816\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61721\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 0.0402 - acc: 0.9855 - val_loss: 1.9659 - val_acc: 0.5675\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61721\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9917\n",
      "Testing Accuracy:  0.5675\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Yoon Kim model (padding = 'same') + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 578, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 192, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 64, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,984,268\n",
      "Trainable params: 8,984,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 136s 11ms/step - loss: 1.0030 - acc: 0.5354 - val_loss: 0.9029 - val_acc: 0.6313\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63131, saving model to ../model_checkpoints/polarity/cnn-w2v-padding-same-2019-06-04T12:06:03.234815.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 153s 13ms/step - loss: 0.6074 - acc: 0.7634 - val_loss: 0.9282 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.63131\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 137s 11ms/step - loss: 0.2275 - acc: 0.9194 - val_loss: 1.2543 - val_acc: 0.5883\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.63131\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.0966 - acc: 0.9689 - val_loss: 1.7408 - val_acc: 0.5757\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63131\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 135s 11ms/step - loss: 0.0504 - acc: 0.9833 - val_loss: 1.9429 - val_acc: 0.5801\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.63131\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-w2v-padding-same'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Yoon Kim model + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = KeyedVectors.load_word2vec_format('../embeddings/glove-sbwc.i25.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = glove_vectors[word]\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        glove_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 576, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 189, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 59, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,984,268\n",
      "Trainable params: 8,984,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 130s 11ms/step - loss: 0.9480 - acc: 0.5785 - val_loss: 0.8520 - val_acc: 0.6476\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64763, saving model to ../model_checkpoints/polarity/cnn-glove-2019-06-04T12:25:03.286500.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 132s 11ms/step - loss: 0.6512 - acc: 0.7405 - val_loss: 0.8910 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64763\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 132s 11ms/step - loss: 0.3063 - acc: 0.8910 - val_loss: 1.0914 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64763\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 138s 11ms/step - loss: 0.1325 - acc: 0.9568 - val_loss: 1.5486 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64763\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 138s 11ms/step - loss: 0.0641 - acc: 0.9814 - val_loss: 1.5703 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64763\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9898\n",
      "Testing Accuracy:  0.6187\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (padding 'same') + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 578, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 64, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,984,268\n",
      "Trainable params: 8,984,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 132s 11ms/step - loss: 0.9345 - acc: 0.5820 - val_loss: 0.8761 - val_acc: 0.6372\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63724, saving model to ../model_checkpoints/polarity/cnn-glove-padding-same-2019-06-04T12:41:03.316601.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 133s 11ms/step - loss: 0.6315 - acc: 0.7498 - val_loss: 0.9105 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63724 to 0.64466, saving model to ../model_checkpoints/polarity/cnn-glove-padding-same-2019-06-04T12:41:03.316601.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.2857 - acc: 0.8944 - val_loss: 1.1604 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64466\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.1212 - acc: 0.9617 - val_loss: 1.4062 - val_acc: 0.6135\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64466\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.0555 - acc: 0.9851 - val_loss: 1.7002 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64466\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 133s 11ms/step - loss: 0.0334 - acc: 0.9894 - val_loss: 1.9989 - val_acc: 0.6165\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.64466\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-glove-padding-same'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          8346900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,507,603\n",
      "Trainable params: 8,507,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11192 samples, validate on 1244 samples\n",
      "Epoch 1/5\n",
      "11192/11192 [==============================] - 216s 19ms/step - loss: 1.0517 - acc: 0.4335 - val_loss: 1.0553 - val_acc: 0.4341\n",
      "Epoch 2/5\n",
      "11192/11192 [==============================] - 225s 20ms/step - loss: 1.0507 - acc: 0.4372 - val_loss: 1.0546 - val_acc: 0.4341\n",
      "Epoch 3/5\n",
      "11192/11192 [==============================] - 231s 21ms/step - loss: 1.0505 - acc: 0.4336 - val_loss: 1.0543 - val_acc: 0.4341\n",
      "Epoch 4/5\n",
      "11192/11192 [==============================] - 219s 20ms/step - loss: 1.0503 - acc: 0.4372 - val_loss: 1.0547 - val_acc: 0.4341\n",
      "Epoch 5/5\n",
      "11192/11192 [==============================] - 212s 19ms/step - loss: 1.0497 - acc: 0.4372 - val_loss: 1.0557 - val_acc: 0.4341\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,878,503\n",
      "Trainable params: 7,878,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 159s 16ms/step - loss: 1.0539 - acc: 0.4331 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0503 - acc: 0.4316 - val_loss: 1.0498 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 165s 17ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0522 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 168s 17ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0489 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 173s 17ms/step - loss: 1.0508 - acc: 0.4344 - val_loss: 1.0492 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4344\n",
      "Testing Accuracy:  0.4469\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 578, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,891,596\n",
      "Trainable params: 8,891,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 331s 27ms/step - loss: 0.9755 - acc: 0.5545 - val_loss: 0.8582 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63947, saving model to ../model_checkpoints/polarity/biLSTM-2019-06-04T12:57:07.503644.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 330s 27ms/step - loss: 0.5536 - acc: 0.7959 - val_loss: 0.9465 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.63947\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 335s 28ms/step - loss: 0.2168 - acc: 0.9267 - val_loss: 1.2999 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.63947\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 344s 28ms/step - loss: 0.0982 - acc: 0.9704 - val_loss: 1.4427 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63947\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 330s 27ms/step - loss: 0.0585 - acc: 0.9824 - val_loss: 1.6469 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.63947\n"
     ]
    }
   ],
   "source": [
    "model_name = 'biLSTM'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 578, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,891,596\n",
      "Trainable params: 8,891,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 330s 27ms/step - loss: 0.9832 - acc: 0.5573 - val_loss: 0.8543 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64318, saving model to ../model_checkpoints/polarity/bilstm-w2v-2019-06-04T13:26:10.587398.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 334s 28ms/step - loss: 0.6020 - acc: 0.7679 - val_loss: 0.8965 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64318\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 337s 28ms/step - loss: 0.2680 - acc: 0.9094 - val_loss: 1.1728 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64318\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 341s 28ms/step - loss: 0.1176 - acc: 0.9626 - val_loss: 1.4839 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64318\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 335s 28ms/step - loss: 0.0672 - acc: 0.9795 - val_loss: 1.7182 - val_acc: 0.5801\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64318\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bilstm-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 578, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,891,596\n",
      "Trainable params: 8,891,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 344s 28ms/step - loss: 0.9163 - acc: 0.5970 - val_loss: 0.8276 - val_acc: 0.6491\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64911, saving model to ../model_checkpoints/polarity/bilstm-glove-2019-06-04T13:57:37.018938.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 350s 29ms/step - loss: 0.6463 - acc: 0.7444 - val_loss: 0.8090 - val_acc: 0.6677\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64911 to 0.66766, saving model to ../model_checkpoints/polarity/bilstm-glove-2019-06-04T13:57:37.018938.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 352s 29ms/step - loss: 0.3463 - acc: 0.8744 - val_loss: 0.9961 - val_acc: 0.6521\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.66766\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 342s 28ms/step - loss: 0.1512 - acc: 0.9511 - val_loss: 1.2563 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.66766\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 338s 28ms/step - loss: 0.0729 - acc: 0.9782 - val_loss: 1.4715 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.66766\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 337s 28ms/step - loss: 0.0477 - acc: 0.9846 - val_loss: 1.6747 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.66766\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bilstm-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               120300    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,838,403\n",
      "Trainable params: 7,838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length),\n",
    "    layers.GRU(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 130s 13ms/step - loss: 1.0530 - acc: 0.4282 - val_loss: 1.0500 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 132s 13ms/step - loss: 1.0518 - acc: 0.4289 - val_loss: 1.0511 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 126s 13ms/step - loss: 1.0506 - acc: 0.4306 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 133s 13ms/step - loss: 1.0505 - acc: 0.4344 - val_loss: 1.0515 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 130s 13ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0520 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 578, 300)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 100)               120300    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,838,403\n",
      "Trainable params: 7,838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.SpatialDropout1D(0.2),\n",
    "    layers.GRU(64),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0513 - acc: 0.4335 - val_loss: 1.0488 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 156s 16ms/step - loss: 1.0521 - acc: 0.4344 - val_loss: 1.0512 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 153s 15ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0586 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 169s 17ms/step - loss: 1.0506 - acc: 0.4344 - val_loss: 1.0507 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0503 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,844,876\n",
      "Trainable params: 8,844,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.9822 - acc: 0.5555 - val_loss: 0.8561 - val_acc: 0.6335\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63353, saving model to ../model_checkpoints/polarity/bigru-w2v-2019-06-04T14:32:32.358771.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.5851 - acc: 0.7739 - val_loss: 0.8931 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63353 to 0.64243, saving model to ../model_checkpoints/polarity/bigru-w2v-2019-06-04T14:32:32.358771.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 276s 23ms/step - loss: 0.2375 - acc: 0.9180 - val_loss: 1.1556 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64243\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.0989 - acc: 0.9701 - val_loss: 1.4688 - val_acc: 0.5861\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64243\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.0564 - acc: 0.9835 - val_loss: 1.6570 - val_acc: 0.5972\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64243\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.0374 - acc: 0.9886 - val_loss: 1.8158 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.64243\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,844,876\n",
      "Trainable params: 8,844,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.9129 - acc: 0.6033 - val_loss: 0.8218 - val_acc: 0.6484\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64837, saving model to ../model_checkpoints/polarity/bigru-glove-2019-06-04T15:49:00.634142.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.6249 - acc: 0.7516 - val_loss: 0.8350 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64837\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 293s 24ms/step - loss: 0.3197 - acc: 0.8912 - val_loss: 0.9704 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64837\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.1303 - acc: 0.9611 - val_loss: 1.2708 - val_acc: 0.6076\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64837\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.0678 - acc: 0.9813 - val_loss: 1.4299 - val_acc: 0.6046\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64837\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN +  LSTM + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 574, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 143, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 7,840,323\n",
      "Trainable params: 7,840,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 106s 11ms/step - loss: 1.0538 - acc: 0.4319 - val_loss: 1.0491 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 108s 11ms/step - loss: 1.0502 - acc: 0.4344 - val_loss: 1.0522 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 108s 11ms/step - loss: 1.0506 - acc: 0.4344 - val_loss: 1.0500 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 108s 11ms/step - loss: 1.0501 - acc: 0.4344 - val_loss: 1.0490 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 107s 11ms/step - loss: 1.0505 - acc: 0.4325 - val_loss: 1.0492 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + CNN + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 574, 64)           41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 8,889,804\n",
      "Trainable params: 8,889,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 308s 25ms/step - loss: 0.9646 - acc: 0.5644 - val_loss: 0.8728 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61795, saving model to ../model_checkpoints/polarity/bigru-cnn-w2v-2019-06-04T16:15:32.057807.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 309s 25ms/step - loss: 0.5510 - acc: 0.7898 - val_loss: 0.9550 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61795 to 0.61944, saving model to ../model_checkpoints/polarity/bigru-cnn-w2v-2019-06-04T16:15:32.057807.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 309s 25ms/step - loss: 0.2001 - acc: 0.9302 - val_loss: 1.3415 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61944\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 311s 26ms/step - loss: 0.0870 - acc: 0.9714 - val_loss: 1.6379 - val_acc: 0.6001\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61944\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 321s 26ms/step - loss: 0.0529 - acc: 0.9820 - val_loss: 1.8591 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61944\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 320s 26ms/step - loss: 0.0355 - acc: 0.9866 - val_loss: 1.9790 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61944\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-cnn-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + CNN + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 574, 64)           41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 8,889,804\n",
      "Trainable params: 8,889,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 315s 26ms/step - loss: 0.9176 - acc: 0.5993 - val_loss: 0.8457 - val_acc: 0.6454\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64540, saving model to ../model_checkpoints/polarity/bigru-cnn-glove-2019-06-04T16:48:58.751815.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 311s 26ms/step - loss: 0.6409 - acc: 0.7435 - val_loss: 0.8538 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64540 to 0.65059, saving model to ../model_checkpoints/polarity/bigru-cnn-glove-2019-06-04T16:48:58.751815.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 311s 26ms/step - loss: 0.3164 - acc: 0.8862 - val_loss: 1.1413 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65059\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 315s 26ms/step - loss: 0.1300 - acc: 0.9572 - val_loss: 1.4410 - val_acc: 0.6165\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.65059\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 313s 26ms/step - loss: 0.0690 - acc: 0.9786 - val_loss: 1.7066 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.65059\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 308s 25ms/step - loss: 0.0390 - acc: 0.9875 - val_loss: 2.0310 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.65059\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-cnn-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/polarity-extended.csv', sep=\";\")\n",
    "df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm   \n",
       "1                        RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet   \n",
       "2  Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh   \n",
       "3  RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…   \n",
       "4       Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faltan dias acabe pesadilla llamada michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hace pocos días amigos masificaron imagen supuesto vino michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                        abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia   \n",
       "1                                            faltan dias acabe pesadilla llamada michelle bachelet   \n",
       "2               michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo   \n",
       "3                        hace pocos días amigos masificaron imagen supuesto vino michelle bachelet   \n",
       "4  alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(text_preprocessing.normalize, no_tweet_hashtags=True, no_camel_case=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        993\n",
       "polarity    993\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.text.str.contains('bachelet')]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6016320474777448\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "455px",
    "width": "241px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "288.85px",
    "left": "205px",
    "right": "361px",
    "top": "177px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
