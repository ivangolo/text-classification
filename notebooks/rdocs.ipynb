{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 19,  4,  5, 20,  2,  3,  1, 29,  7,  8,  9, 10, 11, 12, 13, 14,\n",
       "       15, 16, 17, 18, 42, 41, 52, 50, 51, 53, 58, 56, 47, 91, 94, 28, 48,\n",
       "       87, 38])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/rdocs.csv', sep=\",\")\n",
    "df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>Conchay Toro       Gerencia Agrícola y Técnica     011586     CONTRATO DE RETIROS DE ORUJO Y ESCOBAJOS   VENDIMIA 2014     En Santiago de Chile, a 01 de Febrero de 2014, entre la sociedad VIÑA CONCHA Y   TORO S.A., RUT 90.227.000-0, representada por el señor OSVALDO SOLAR   VENEGAS, RUT 9.002.083-8 con domicilio en Avenida Nueva Tajamar N°481 Piso 15,   Las Condes, Santiago, en adelante indistintamente “El Contratante4   44 o, simplemente,   Concha y Toro; y por otra parte a MAQUINARIAS Y SE...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>D10021       ADDENDUM   CONTRATO DE PRESTACIÓN DE SERVICIOS DE INVESTIGACIÓN Y DESARROLLO   ACOGIDOS AL BENEFICIO TRIBUTARIO DE LA LEY 20.241.   ENTRE   VIÑA CONCHA Y TORO S.A.   Y   FUNDACIÓN CHILE   PROYECTO HUELLA DE AGUA — CONCHA Y TORO:   INVESTIGACIÓN Y DESARROLLO DE MEDICIÓN DE HUELLA DE AGUA CORPORATIVA   Y ANÁLISIS DE IMPACTO       En Sanliago, con fecha 10 de Marzo de 2010, entre la empresa VIÑA CONCHA Y TORO S.A.,   RUT N°90.227.000-0 en adelante “CyT”, representada por los señore...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>010457       V-h     pal®   Concha yToro     Gerencia Agrícola y Técnica     CONTRATO DE RETIROS DE ORUJO Y ESCOBAJOS   VENDIMIA 2011       En Santiago de Chile, a 01 de Febrero de 2011, entre la sociedad VIÑA CONCHA Y   TORO S.A., RUT 90.227.000-0, representada por el señor OSVALDO SOLAR   VENEGAS, RUT 9.002.083-8 con domicilio en Avenida Nueva Tajamar N°481 Piso 15,   Las Condes, Santiago, en adelante indistintamente “El Contratante4   44 o, simplemente,   Concha y Toro; y por otra parte a...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215</td>\n",
       "      <td>ECOLAB\"   01Ó002       CONTRATO PRESTACION DE SERVICIOS   DE MANEJO INTEGRADO DE PLAGAS       En Santiago, República de Chile, a 21 de Diciembre de 2009, comparecen por una parte ECOLAB   S.A., RUT: 96.604.460-8, representado por don Carlos Emilio Oglio, RUT: 23.002.718-8, y don Juan   Pablo Romero Muñoz, RUT: 12.464.201-9, domiciliados en Av. Pedro de Valdivia 3801, comuna deÑuñoa, eñ adelante “la empresa” y por la otra: Viña Concha y Toro S.A. RUT: 90.227.000 -0 ,   representada por don(ña...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>010115 GE-13       CEPIA INGENIEROS CONSULTORES LIMITADA     CONTRATO DE SERVICIOS N°       INSCRIPCIÓN DERECHOS DE APROVECHAMIENTO DE AGUAS   SUBTERRÁNEAS       En Talca, a 26 de Mayo de 2010 entre Viña Concha y Toro S.A. R.U.T.   90.227.000-0, representada por Jorge Andrés Larraín Santa María y Osvaldo   Rafael Solar Venegas, R.U.T. 4.330.116-0, 9.002.083-8, ambos con domicilio en   Nueva Tajamar N° 481 Torre Norte Piso N°15 Las Condes, en adelante “El   Cliente” y CEPIA INGENIEROS CONSULT...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  \\\n",
       "0  212   \n",
       "1  213   \n",
       "2  214   \n",
       "3  215   \n",
       "4  216   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Text  \\\n",
       "0  Conchay Toro       Gerencia Agrícola y Técnica     011586     CONTRATO DE RETIROS DE ORUJO Y ESCOBAJOS   VENDIMIA 2014     En Santiago de Chile, a 01 de Febrero de 2014, entre la sociedad VIÑA CONCHA Y   TORO S.A., RUT 90.227.000-0, representada por el señor OSVALDO SOLAR   VENEGAS, RUT 9.002.083-8 con domicilio en Avenida Nueva Tajamar N°481 Piso 15,   Las Condes, Santiago, en adelante indistintamente “El Contratante4   44 o, simplemente,   Concha y Toro; y por otra parte a MAQUINARIAS Y SE...   \n",
       "1  D10021       ADDENDUM   CONTRATO DE PRESTACIÓN DE SERVICIOS DE INVESTIGACIÓN Y DESARROLLO   ACOGIDOS AL BENEFICIO TRIBUTARIO DE LA LEY 20.241.   ENTRE   VIÑA CONCHA Y TORO S.A.   Y   FUNDACIÓN CHILE   PROYECTO HUELLA DE AGUA — CONCHA Y TORO:   INVESTIGACIÓN Y DESARROLLO DE MEDICIÓN DE HUELLA DE AGUA CORPORATIVA   Y ANÁLISIS DE IMPACTO       En Sanliago, con fecha 10 de Marzo de 2010, entre la empresa VIÑA CONCHA Y TORO S.A.,   RUT N°90.227.000-0 en adelante “CyT”, representada por los señore...   \n",
       "2  010457       V-h     pal®   Concha yToro     Gerencia Agrícola y Técnica     CONTRATO DE RETIROS DE ORUJO Y ESCOBAJOS   VENDIMIA 2011       En Santiago de Chile, a 01 de Febrero de 2011, entre la sociedad VIÑA CONCHA Y   TORO S.A., RUT 90.227.000-0, representada por el señor OSVALDO SOLAR   VENEGAS, RUT 9.002.083-8 con domicilio en Avenida Nueva Tajamar N°481 Piso 15,   Las Condes, Santiago, en adelante indistintamente “El Contratante4   44 o, simplemente,   Concha y Toro; y por otra parte a...   \n",
       "3  ECOLAB\"   01Ó002       CONTRATO PRESTACION DE SERVICIOS   DE MANEJO INTEGRADO DE PLAGAS       En Santiago, República de Chile, a 21 de Diciembre de 2009, comparecen por una parte ECOLAB   S.A., RUT: 96.604.460-8, representado por don Carlos Emilio Oglio, RUT: 23.002.718-8, y don Juan   Pablo Romero Muñoz, RUT: 12.464.201-9, domiciliados en Av. Pedro de Valdivia 3801, comuna deÑuñoa, eñ adelante “la empresa” y por la otra: Viña Concha y Toro S.A. RUT: 90.227.000 -0 ,   representada por don(ña...   \n",
       "4  010115 GE-13       CEPIA INGENIEROS CONSULTORES LIMITADA     CONTRATO DE SERVICIOS N°       INSCRIPCIÓN DERECHOS DE APROVECHAMIENTO DE AGUAS   SUBTERRÁNEAS       En Talca, a 26 de Mayo de 2010 entre Viña Concha y Toro S.A. R.U.T.   90.227.000-0, representada por Jorge Andrés Larraín Santa María y Osvaldo   Rafael Solar Venegas, R.U.T. 4.330.116-0, 9.002.083-8, ambos con domicilio en   Nueva Tajamar N° 481 Torre Norte Piso N°15 Las Condes, en adelante “El   Cliente” y CEPIA INGENIEROS CONSULT...   \n",
       "\n",
       "   Type  \n",
       "0     6  \n",
       "1     6  \n",
       "2     6  \n",
       "3     6  \n",
       "4     6  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID      1580\n",
       "Text    1580\n",
       "Type    1580\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>191</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>170</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  Text\n",
       "Type           \n",
       "1     120   120\n",
       "2      66    66\n",
       "3      27    27\n",
       "4      16    16\n",
       "5      30    30\n",
       "6      40    40\n",
       "7      98    98\n",
       "8     191   191\n",
       "9      24    24\n",
       "10     53    53\n",
       "11     75    75\n",
       "12     49    49\n",
       "13     28    28\n",
       "14     52    52\n",
       "15     34    34\n",
       "16     45    45\n",
       "17     61    61\n",
       "18     40    40\n",
       "19    170   170\n",
       "20     39    39\n",
       "28      3     3\n",
       "29    136   136\n",
       "38      6     6\n",
       "41     22    22\n",
       "42     22    22\n",
       "47     25    25\n",
       "48      3     3\n",
       "50     19    19\n",
       "51     12    12\n",
       "52     15    15\n",
       "53     16    16\n",
       "56     15    15\n",
       "58     17    17\n",
       "87      2     2\n",
       "91      7     7\n",
       "94      2     2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(text_preprocessing.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212</td>\n",
       "      <td>conchay toro gerencia agrícola técnica contrato retiros orujo escobajos vendimia santiago chile febrero sociedad viñ concha toro s rut representada señor osvaldo solar venegas rut domicilio avenida nueva tajamar n° piso condes santiago adelante indistintamente contratante simplemente concha toro parte maquinarias servicios jose diaz urra hijos limitada rut representada don jose vicente diaz urra rut domiciliado cachapoal b peumo adelante transportista ambas compañías mencionadas anteriorment...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>213</td>\n",
       "      <td>d addendum contrato prestació n servicios investigació n desarrollo acogidos beneficio tributario ley viñ concha toro s fundació n chile proyecto huella agua — concha toro investigació n desarrollo medició n huella agua corporativa aná lisis impacto sanliago fecha marzo empresa viñ concha toro s rut n° adelante cy t representada señores eduardo guilisasti gana c n i sr osvaldo solar venegas c n i cuya dirección nueva tajamar n° torre norte pisó condes santiago región metropolitana fundacion ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>214</td>\n",
       "      <td>vh pal concha toro gerencia agrícola técnica contrato retiros orujo escobajos vendimia santiago chile febrero sociedad viñ concha toro s rut representada señor osvaldo solar venegas rut domicilio avenida nueva tajamar n° piso condes santiago adelante indistintamente contratante simplemente concha toro parte jose ignacio fredes cruz rut domiciliados villa suplementeros florentino gerreros santa cruz adelante transportista ambas compañías mencionadas anteriormente celebra contrato retiro orujo...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215</td>\n",
       "      <td>ecolab ó contrato prestacion servicios manejo integrado plagas santiago república chile diciembre comparecen parte ecolab s rut representado don carlos emilio oglio rut don juan pablo romero muñoz rut domiciliados av pedro valdivia comuna deñuñoa eñ adelante empresa viña concha toro s rut representada donña osvaldo solar venegas rut domiciliados av nueva tajamar torre norte piso comuna condes adelante contratante expresan convenido siguiente contrato prestación servicios primero empresa dedi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>ge cepia ingenieros consultores limitada contrato servicios n° inscripció n derechos aprovechamiento aguas subterrá neas talca mayo viña concha toro s r u t representada jorge andrés larraín santa maría osvaldo rafael solar venegas r u t ambos domicilio nueva tajamar n° torre norte piso n° condes adelante cliente cepia ingenieros consultores ltda r u t k representada don marcelo mourgues schurter ingeniero civil r u t ambos domicilio longitudinal sur n° talca adelante empresa convenido sigui...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  \\\n",
       "0  212   \n",
       "1  213   \n",
       "2  214   \n",
       "3  215   \n",
       "4  216   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Text  \\\n",
       "0  conchay toro gerencia agrícola técnica contrato retiros orujo escobajos vendimia santiago chile febrero sociedad viñ concha toro s rut representada señor osvaldo solar venegas rut domicilio avenida nueva tajamar n° piso condes santiago adelante indistintamente contratante simplemente concha toro parte maquinarias servicios jose diaz urra hijos limitada rut representada don jose vicente diaz urra rut domiciliado cachapoal b peumo adelante transportista ambas compañías mencionadas anteriorment...   \n",
       "1  d addendum contrato prestació n servicios investigació n desarrollo acogidos beneficio tributario ley viñ concha toro s fundació n chile proyecto huella agua — concha toro investigació n desarrollo medició n huella agua corporativa aná lisis impacto sanliago fecha marzo empresa viñ concha toro s rut n° adelante cy t representada señores eduardo guilisasti gana c n i sr osvaldo solar venegas c n i cuya dirección nueva tajamar n° torre norte pisó condes santiago región metropolitana fundacion ...   \n",
       "2  vh pal concha toro gerencia agrícola técnica contrato retiros orujo escobajos vendimia santiago chile febrero sociedad viñ concha toro s rut representada señor osvaldo solar venegas rut domicilio avenida nueva tajamar n° piso condes santiago adelante indistintamente contratante simplemente concha toro parte jose ignacio fredes cruz rut domiciliados villa suplementeros florentino gerreros santa cruz adelante transportista ambas compañías mencionadas anteriormente celebra contrato retiro orujo...   \n",
       "3  ecolab ó contrato prestacion servicios manejo integrado plagas santiago república chile diciembre comparecen parte ecolab s rut representado don carlos emilio oglio rut don juan pablo romero muñoz rut domiciliados av pedro valdivia comuna deñuñoa eñ adelante empresa viña concha toro s rut representada donña osvaldo solar venegas rut domiciliados av nueva tajamar torre norte piso comuna condes adelante contratante expresan convenido siguiente contrato prestación servicios primero empresa dedi...   \n",
       "4  ge cepia ingenieros consultores limitada contrato servicios n° inscripció n derechos aprovechamiento aguas subterrá neas talca mayo viña concha toro s r u t representada jorge andrés larraín santa maría osvaldo rafael solar venegas r u t ambos domicilio nueva tajamar n° torre norte piso n° condes adelante cliente cepia ingenieros consultores ltda r u t k representada don marcelo mourgues schurter ingeniero civil r u t ambos domicilio longitudinal sur n° talca adelante empresa convenido sigui...   \n",
       "\n",
       "   Type  \n",
       "0     6  \n",
       "1     6  \n",
       "2     6  \n",
       "3     6  \n",
       "4     6  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID      1343\n",
       "Text    1343\n",
       "Type    1343\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID      237\n",
       "Text    237\n",
       "Type    237\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.Text.values\n",
    "y_train = train['Type'].values\n",
    "X_test = test.Text.values\n",
    "y_test = test['Type'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.34177215189873417\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.53      0.60        17\n",
      "           2       0.80      0.80      0.80        10\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         4\n",
      "           5       1.00      0.71      0.83         7\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00        18\n",
      "           8       0.18      1.00      0.31        25\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00        10\n",
      "          11       0.00      0.00      0.00        12\n",
      "          12       0.00      0.00      0.00         5\n",
      "          13       0.00      0.00      0.00         3\n",
      "          14       0.00      0.00      0.00        11\n",
      "          15       0.00      0.00      0.00         9\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00        11\n",
      "          18       0.00      0.00      0.00         5\n",
      "          19       0.40      1.00      0.57        24\n",
      "          20       1.00      0.71      0.83         7\n",
      "          28       0.00      0.00      0.00         1\n",
      "          29       0.83      0.28      0.42        18\n",
      "          41       0.00      0.00      0.00         3\n",
      "          42       0.00      0.00      0.00         6\n",
      "          47       0.00      0.00      0.00         4\n",
      "          50       0.00      0.00      0.00         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         2\n",
      "          58       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.34       237\n",
      "   macro avg       0.16      0.16      0.14       237\n",
      "weighted avg       0.27      0.34      0.25       237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8016877637130801\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      1.00      0.83        17\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.25      0.40         4\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       0.62      1.00      0.77         5\n",
      "           7       0.76      0.72      0.74        18\n",
      "           8       0.86      1.00      0.93        25\n",
      "           9       1.00      0.80      0.89         5\n",
      "          10       1.00      0.70      0.82        10\n",
      "          11       0.85      0.92      0.88        12\n",
      "          12       1.00      0.60      0.75         5\n",
      "          13       0.38      1.00      0.55         3\n",
      "          14       1.00      0.45      0.62        11\n",
      "          15       0.86      0.67      0.75         9\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       1.00      0.55      0.71        11\n",
      "          18       0.67      0.80      0.73         5\n",
      "          19       0.75      1.00      0.86        24\n",
      "          20       0.86      0.86      0.86         7\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       0.76      0.89      0.82        18\n",
      "          41       0.50      0.33      0.40         3\n",
      "          42       1.00      0.83      0.91         6\n",
      "          47       1.00      1.00      1.00         4\n",
      "          50       0.67      0.67      0.67         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         2\n",
      "          58       0.75      0.75      0.75         4\n",
      "\n",
      "    accuracy                           0.80       237\n",
      "   macro avg       0.70      0.67      0.66       237\n",
      "weighted avg       0.81      0.80      0.78       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suport Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SVC(class_weight='balanced')),\n",
    "               ])\n",
    "svm_rbf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_rbf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.029535864978902954\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=1, penalty='l2',\n",
       "                                    random_state=None, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8143459915611815\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.88      0.81        17\n",
      "           2       0.83      1.00      0.91        10\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      0.50      0.67         4\n",
      "           5       1.00      0.86      0.92         7\n",
      "           6       0.71      1.00      0.83         5\n",
      "           7       0.68      0.72      0.70        18\n",
      "           8       0.89      0.96      0.92        25\n",
      "           9       1.00      0.80      0.89         5\n",
      "          10       0.89      0.80      0.84        10\n",
      "          11       0.83      0.83      0.83        12\n",
      "          12       0.75      0.60      0.67         5\n",
      "          13       0.38      1.00      0.55         3\n",
      "          14       1.00      0.64      0.78        11\n",
      "          15       0.88      0.78      0.82         9\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       1.00      0.73      0.84        11\n",
      "          18       0.80      0.80      0.80         5\n",
      "          19       0.89      1.00      0.94        24\n",
      "          20       1.00      0.86      0.92         7\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       0.80      0.89      0.84        18\n",
      "          41       0.50      0.67      0.57         3\n",
      "          42       1.00      0.83      0.91         6\n",
      "          47       1.00      1.00      1.00         4\n",
      "          50       0.50      0.33      0.40         3\n",
      "          51       0.00      0.00      0.00         1\n",
      "          52       0.00      0.00      0.00         1\n",
      "          53       0.00      0.00      0.00         2\n",
      "          56       0.00      0.00      0.00         2\n",
      "          58       0.60      0.75      0.67         4\n",
      "          91       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.81       237\n",
      "   macro avg       0.68      0.66      0.66       237\n",
      "weighted avg       0.82      0.81      0.81       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model_checkpoints/rdocs/logreg.joblib']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving model\n",
    "joblib.dump(logreg, '../model_checkpoints/rdocs/logreg.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8143459915611815\n"
     ]
    }
   ],
   "source": [
    "clf2 = joblib.load('../model_checkpoints/rdocs/logreg.joblib')\n",
    "y_pred = clf2.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237,)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving Vectorizer\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "CHECKPOINTS_PATH = '../model_checkpoints/rdocs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train['Type']).values\n",
    "y_test = pd.get_dummies(test['Type']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'santiago testimonio escritura insercion acta poderes sociedad inversiones servicios construccion s pronf an iiorovitz jorge r santiago chile abril •••••••••••« »«•••••••••••••¦•c «fry« ›«¦•••«o¦•¦•••»•••¦•••xtwo•x t s notaria enrique morgan torres agustinas telefono z x c rep i insercion acta poderes • • • • • • • • sociedad inversiones servicios construccion b bronfman horovitz jorge • • • • • • • • santiago chile siete abril mil novecientos ochenta ocho enrique morgan torres abogado notario titular segunda notaria santiago oficio agustinas mil ciento once ciudad comparecen don fernando perez bucchi chileno casado ingeniero civil oédula identidad número dos millones sesenta cuatro mil sesenta cinco raya cero santiago don jaime reyes gutierrez chileno casado contador general cédula identidad número millón novecientos veintiseis mil ochocientos noventa cuatro raya dos santiago calidades vicepresidente gerente general respectivamente sociedad inversiones servicios construccion s domicilio calle huérfanos mil cincuenta dos piso nueve mayores edad acreditan identidad cédulas citadas exponen debidamente facultados vienen reducir escritura pública partes pertinentes siguiente acta septuagesima segunda sesion directorio sociedad inversio nes servicios construccion s celebrada lunes x c veintisiete julio mil novecientos ochenta siete abre seal n • quince treinta horas presidida titular olor jorge bronfman h asistencia vicepresidente señor fernando pares b direc tores seftores jaime allende u ernesto fleischmann c máximo honorato sergio kohn p sergio nay c sergio melo san juan sergio orellana s gerente general señor jaime reyes g poderes olor presidente sena necesidad sociedad otorgue nuevo texto poderes representaci n actos contratos operaciones ejecute celebre objeto permitir expedita operatoria ejecución desarrollo negocios sociales después debate particular directorio unanimidad acordó unp designacion mandatarios existirán dos tipos manda tarios mandatarios clase presiden vicepresidente gerente general socie dad mandatarios clase restantes directores sociedad dos conferir siguientes poderes representtción sociedad inversio nes servicios construcción s invesco prime ro acuerdo previo directorio sociedad obligación rendir cuenta posterior éste • necesario acreditar terceros dicho acuer do directorio rendición cuenta éste • mandatarios clase actuando conjuntamente dos cuales quiera siguientes facultades comprar adquirir cualquier forma prometer comprar adquirir bienes inmuebles derechos constituidos éstos vender general enajenar prometer x c rc r • v pdal vs enajenación bienes inmuebles derechos dos éstos b vender general enajenar prometer enajenación acciones emitidas socieda des filiales ceder derechos sociales éstas dar bienes hipotecas incluso cláusula garantía general dar prenda bienes muebles valores mobi liarios derechos acciones demás cosas corporales • incorporales prenda civil mercantil bancaria desplazamiento agraria industrial warrants cosas muebles vendidas plazo u especiales gravar bienes sociedad derechos uso usufructo habitación prohibiciones servidumbres acti vas pasivas cualquiera limitación domi nio segundo mandatarios clase actuando conjun tamente dos cualesquiera siguientes facultades comprar vender permutar general adquirir enajenar cualquier titulo toda clase bienes muebles corporales • incorporales incluso accio nes bonos debentures toda clase valores mobilia rios títulos decrádito efectos públicos comer cio b dar tomar arrendamiento administración concesión toda clase bienes muebles inmuebles obte ner concesiones administrativas cualquier naturaleza u objeto toda clase bienes incluso inmuebles c representar sociedad vos voto sociedades comunidades asociaciones cuentas partici pación sociedades hecho etcétera parte interés asistir representación juntas accionistas socios comuneros asociados tales entidades modificar dotas pedir x c disolución terminación incluso anticipada expresar intención no prorrogarlas pedir liquidación partición llevar cabo unas general ejercitar renunciar todas acciones derechos cumplir todas obligaciones sociedad correspondan socia accionista gestora comandita ria comunera asociada liquidadora etcétera tales comunidades asociaciones u tipo entidades dar dinero bienes mutuo ceder aceptar cesiones crédito derechos nominativos orden portador garantías reales personales celebrar contratos seguros comodato transacción aún respecto cosas no disputadas depósito necesario voluntario transporte fletamento cambio correduría administración agencia comisi n representación distribu ción concesión conviniendo toda clase pactos estipulaciones no contemplados especial mente leyes esencia naturale za meramente accidentares desahuciar anular res cindir resciliar resolver revooar terminar solici tar terminación actos contratos sociedad hubiere celebrado f pedir rendiciones cuen tas aprobarlas u objetarlas convenir aceptar estima ciones perjuicios cláusulas penales multas g pa gar hoyar remitir extinguir cualquier modo toda clase obligaciones sociedad h representar sociedad bancos instituciones financieras nacionales extranjeros particulares estatales amplias facultades darles instrucciones come x c tenles comisiones confianza abrir cerró taco corrientes bancarias mercantiles depósito cré dito ahorro imponerse movimientos depositar girar sobregirar reconocer impugnar saldos retirar libretos cheques cheques sueltos tomar dinero mutuo contratar préstamos quier forma toda clase organismos instituciones crédito fomento derecho público privado crédito cuenta corriente créditos simples crédito documenterios avances aceptación sobregi ros créditos cuentas especiales líneas créditos préstamos letras pagarés cualquier forma garantía moneda nacional extranjera establecer convenir todas condiciones cláusulas modalidades mismos k girar revalidar endo sar depositar cobrar cancelar dar orden no pago protestar toda clase cheques girar suscribir endosar dominio cobro garantía restric ciones descontar protestar prorrogar cobrar cance lar transferiry disponer cualquier forma letras cambio pagarés demás efectos negociables comercio bancarios mercantiles nominativos orden portador ejercitar derechos acciones sociedad correspondan relación tales documentos operar amplias facultades dentro mercado capitales pudiendo comprar ven der negociar invertir cualquier forma acciones bonos debenturee pagarés toda clase valores mobiliarios letras cambio efectos comercio general cualquier sistema inversión x c ahorro reajuatable no largo mediano corto plazo vista condicionales actualmente exista país pueda establecerse futuro convenir condiciones precios intereses términos modalidades operaciones firmar documentos llevarlas cabo representar sociedad derecho vos voto amplias facultades entidades emisoras dichos títulos m hacer depósitos plazo bancos instituciones financieras nacionales extranjeros retirarlos entregar retirar valores custodia garantía cobranza contratar cajas seguridad retirar encuentre tomar boletas bancarias pólizas seguros garan tías efectuar toda clase operaciones bancarias n realizar toda clase importaciones exportaciones operaciones cambios internacionales pudiendo ello efectuar todas actuaciones suscribir documentos presentaciones fueren necesarios materializarlas restricción alguna representar sociedad amplias facultades ninguna limitación ministerios servicios públicos tribunales justicia contraloría general repúbli ca superintendencia valores seguros administra dora fondos pensiones servicio impuestos inter corporación fomento producción tesorerías municipalidades intendencias gobernaciones entidades previsión social cualquier organismo institución empresa persona pública privada autónoma fiscal semifiscal municipal particular pu diendo formularles toda clase presentaciones declara x c ¦ u lic ci ciones s in obligatorias peticiones etcétera modificar desistirse p representar sociedad juicios gestiones judiciales quier tribunal ordinario especial arbitral administra tivo cualquier clase así intervenga socie dad demandante demandada tercero cualquier especie pudiendo ejercitar toda clase acciones ordinarias ejecutivas especiales jurisdicción no contenciosa cualquier naturaleza ejercicio representación judicial faculta dos actuar sociedad todas facultades ordinarias extraordinarias mandato judicial términos previstos artículo séptimo código procedimiento civil pudiendo desistirse primera ins tancia acción entablada contestar demandas acep tar demanda contraria renunciar recursos términos legales absolver posiciones transigir compro meter otorgar árbitros facultades arbitradores cobrar percibir prorrogar jurisdicción intervenir gestiones conciliación avenimiento proponer apro bar convenios convenir modificaciones aceptar avenimientos conceder u otorgar quitas esperas conferir mandatos especiales judiciales extrajudicia revocarlos delegar parte faculta des consignan letras precedentes ello signifique pérdida ejercicio mismas dejar efecto delegaciones reasumir tercero mandatarios clase b actuando cualquiera conjunto cualquiera mandatarios clase facultades consignadas punto x c segundo presente acuerdo mandatarios clase b sólo podrán actuar caso ausencia impedimento cualquiera apoderados clase deberán dar cuenta cometido siguiente sesi n directorio no necesario acreditar terceros ausencia impedimento algún apoderado clase así tampoco información debe rendir directorio sociedad cuarto manda tarios clase actuando separadamente cualquiera siguientes facultades celebrar contratos trabajo colectivos individuales tar servicios profesionales técnicos poner término solicitar terminación mismos b cobrar percibir cuanto adeude sociedad aceptar toda clase cauciones garantías beneficio firmar recibos finiquitos cancelaciones procedieren c inscribir propiedad intelectual in dustrial marcas comerciales modelos industriales paten tar inventos deducir oposiciones solicitar nulidades general efectuar todas tramitaciones actuaciones procedentes materias firmar reci bir entregar retirar toda clase correspondencia adn certificada giros cargas reembolsos encomiendas piezas postales etcétera dirigidas consignadas sociedad loa mandataros quedan facultados delegar parte facultades consignadas letra constancia actuales mandatarios directo rio acuerda dejar constancia actualmente cargos presidente vicepresidente gerente general directo res sociedad ocupan siguientes personas x c consiguiente mandatarios clase b según corresponda acuer señalado flamero presente acuerdo denominado designación mandatarios cargos presidente vicepresidente gerente general actualmente desempe nados menores jorge bronfman horovitz fernando pérez bucchi jaime reyes gutierrez respectivamente cargos directores sociedad actualmente desempeñados señores jaime allende urrutia ernes to fleisohmann cahn máximo honorato alarnos sergio may colvin sergio melo san juan sergio orellana salcedo sergio kohn pepay poder especial además efectos previstos articulo cuarenta dos reglamento sociedades anónimas directorio acordó conferir poder presidente vicepresidente socie dad cualquiera ausencia geren general no necesario acreditar terceros puedan representar válidamente sociedad todas notificaciones practiquen revoca cion poderes motivo texto ártico refundido poderes acuerdan revocar dejar efecto contar quince marzo ano mil novecientos ochen ta ocho cualquier sociedad hubiere otorga do anterioridad fecha especial acordado sesión directorio celebrada dos diciembre mil novecientos ochenta cuya acta redujo escritura pública día treinta mismo mes notaria ciudad don enrique morgan torres reduccion escritura publica directorio acordó fa cultar señores fernando pérez bucchi jaime reyes x c eow tx ad sp elesioe ip eaunt sxed óraolo jia els• ep u tunea initx jd v ihrel to olue tw jqmou ele• vsownwoo set eaue tnb p teetlft znl jed s tn ioxeci vel joh • ue ustw ou solas opue alosa ésle tuo tooy sp tsupo slunr wwx jd ilesy •oplounue usqoe s ueolydmee j anb sa joloe j sop j jciwou •peowd pspeloos •y ap solnyele j juo w ose joloena sp seuo tounj u to jere óluenjee s toueoue jitn buis uop enb oye sopo jod ani pnl ile i sp eousleuop opuerep relueulozui ap ud ne ue owolpea ta sp souoses set mol j epsep isoyopulm au t set owoloe j ia ise tounuea sop asma ep soluewspun so eop tpuely seopilynse j uspiownlis ep setwozu t someym loosa ani ep ee joloe j ta owoo asnye ae ap ódme tul uolwat ú rypon jleuo somiles ap iwagy h edijley svpsuo towye j sepeptlue set ap sopslynee j u townl is et sapos s jwva joju i ap u iounj sicio p et elueurseuyltnwpi weluel enb eluswettuoo epexed ou enb ximep isuoo jod joloe j ta sp olimpo tv t iounue ne weluese jd ope tes vuetteao t jes awded ut iox ot ss ge jollig gg b itg gol ia re oinsikyurwon vidniinvi éluewet ee ete ap enb áloe top upowq jcis jo jeleod st owiseoeu ves anb upa u ises oluee jd wy u eopindopir empalmo ovinpewut ap oloeje s aswett paops owolo jia socluanov sol vi oin in ild wu iq d sl jn jose salo u ved ue opol ue uryourel iuevesu enb ixed vormd jnyfaose v anuese jd •l jed ue opo us ulolnpe j ilood je inbreno us eluewsp jdes lunruoo opunlo énb jiu jnolno x c levanta sesión firmado jorge bronfman h filman do pérez jaime allende u ernesto fleischmann c máximo honorato sergio kohn p sergio may c sergio melo s sergio orellana s jaime reyes g conforme original rola fojas ciento treinta • fojas ciento treinta ocho respectivo libro actas vista comprobante previa lectura firman doy fé deja constancia presente escritura figura anotada repertorio bajo número ciento once firmas fernando perez bucchi jaime reyes gutierrez enrique morgan torres notario publico presente copia testimonio fiel original santiago abril x c x c'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 977,\n",
       " 17,\n",
       " 6022,\n",
       " 90,\n",
       " 268,\n",
       " 3,\n",
       " 19,\n",
       " 105,\n",
       " 512,\n",
       " 4,\n",
       " 39938,\n",
       " 813,\n",
       " 39939,\n",
       " 388,\n",
       " 63,\n",
       " 5,\n",
       " 54,\n",
       " 493,\n",
       " 39940,\n",
       " 39941,\n",
       " 39942,\n",
       " 39943,\n",
       " 52,\n",
       " 4,\n",
       " 329,\n",
       " 299,\n",
       " 825,\n",
       " 754,\n",
       " 1620,\n",
       " 1331,\n",
       " 423,\n",
       " 8,\n",
       " 2,\n",
       " 536,\n",
       " 21,\n",
       " 6022,\n",
       " 90,\n",
       " 268,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 3,\n",
       " 19,\n",
       " 105,\n",
       " 512,\n",
       " 53,\n",
       " 3137,\n",
       " 4767,\n",
       " 388,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 39,\n",
       " 5,\n",
       " 54,\n",
       " 28,\n",
       " 493,\n",
       " 1,\n",
       " 26,\n",
       " 40,\n",
       " 32,\n",
       " 299,\n",
       " 825,\n",
       " 754,\n",
       " 148,\n",
       " 27,\n",
       " 125,\n",
       " 482,\n",
       " 329,\n",
       " 5,\n",
       " 437,\n",
       " 1620,\n",
       " 1,\n",
       " 14,\n",
       " 142,\n",
       " 144,\n",
       " 589,\n",
       " 9,\n",
       " 338,\n",
       " 1956,\n",
       " 5705,\n",
       " 112,\n",
       " 133,\n",
       " 249,\n",
       " 188,\n",
       " 39944,\n",
       " 51,\n",
       " 7,\n",
       " 6,\n",
       " 12,\n",
       " 45,\n",
       " 22,\n",
       " 1,\n",
       " 45,\n",
       " 15,\n",
       " 1049,\n",
       " 82,\n",
       " 5,\n",
       " 9,\n",
       " 717,\n",
       " 1084,\n",
       " 3404,\n",
       " 112,\n",
       " 133,\n",
       " 1355,\n",
       " 50,\n",
       " 67,\n",
       " 51,\n",
       " 7,\n",
       " 765,\n",
       " 26,\n",
       " 2682,\n",
       " 1,\n",
       " 78,\n",
       " 30,\n",
       " 22,\n",
       " 1049,\n",
       " 6,\n",
       " 5,\n",
       " 2869,\n",
       " 1530,\n",
       " 283,\n",
       " 50,\n",
       " 1503,\n",
       " 3,\n",
       " 19,\n",
       " 105,\n",
       " 512,\n",
       " 4,\n",
       " 120,\n",
       " 80,\n",
       " 422,\n",
       " 1,\n",
       " 35,\n",
       " 6,\n",
       " 121,\n",
       " 25,\n",
       " 561,\n",
       " 292,\n",
       " 957,\n",
       " 51,\n",
       " 669,\n",
       " 1169,\n",
       " 594,\n",
       " 303,\n",
       " 1860,\n",
       " 1173,\n",
       " 733,\n",
       " 17,\n",
       " 47,\n",
       " 87,\n",
       " 753,\n",
       " 73,\n",
       " 90,\n",
       " 39945,\n",
       " 482,\n",
       " 2172,\n",
       " 75,\n",
       " 3,\n",
       " 6531,\n",
       " 1343,\n",
       " 105,\n",
       " 512,\n",
       " 4,\n",
       " 508,\n",
       " 2075,\n",
       " 8,\n",
       " 2,\n",
       " 420,\n",
       " 277,\n",
       " 1,\n",
       " 26,\n",
       " 40,\n",
       " 28,\n",
       " 3675,\n",
       " 25225,\n",
       " 13,\n",
       " 39,\n",
       " 228,\n",
       " 23,\n",
       " 380,\n",
       " 3808,\n",
       " 125,\n",
       " 5857,\n",
       " 388,\n",
       " 3137,\n",
       " 379,\n",
       " 502,\n",
       " 1530,\n",
       " 154,\n",
       " 338,\n",
       " 19530,\n",
       " 53,\n",
       " 3976,\n",
       " 7823,\n",
       " 25226,\n",
       " 717,\n",
       " 3482,\n",
       " 61,\n",
       " 2334,\n",
       " 8527,\n",
       " 2,\n",
       " 1088,\n",
       " 4768,\n",
       " 362,\n",
       " 5562,\n",
       " 104,\n",
       " 362,\n",
       " 11516,\n",
       " 2,\n",
       " 362,\n",
       " 2047,\n",
       " 242,\n",
       " 176,\n",
       " 362,\n",
       " 2977,\n",
       " 4,\n",
       " 283,\n",
       " 50,\n",
       " 154,\n",
       " 717,\n",
       " 1084,\n",
       " 151,\n",
       " 268,\n",
       " 5857,\n",
       " 124,\n",
       " 39946,\n",
       " 1464,\n",
       " 3,\n",
       " 2683,\n",
       " 678,\n",
       " 950,\n",
       " 268,\n",
       " 4433,\n",
       " 13,\n",
       " 348,\n",
       " 113,\n",
       " 294,\n",
       " 5858,\n",
       " 1835,\n",
       " 97,\n",
       " 5859,\n",
       " 25227,\n",
       " 12535,\n",
       " 411,\n",
       " 619,\n",
       " 684,\n",
       " 85,\n",
       " 1207,\n",
       " 2061,\n",
       " 1788,\n",
       " 75,\n",
       " 288,\n",
       " 401,\n",
       " 19531,\n",
       " 2941,\n",
       " 662,\n",
       " 19532,\n",
       " 6,\n",
       " 12541,\n",
       " 10691,\n",
       " 25228,\n",
       " 662,\n",
       " 55,\n",
       " 9450,\n",
       " 1530,\n",
       " 283,\n",
       " 50,\n",
       " 2613,\n",
       " 1218,\n",
       " 662,\n",
       " 55,\n",
       " 1236,\n",
       " 273,\n",
       " 3,\n",
       " 6,\n",
       " 1135,\n",
       " 128,\n",
       " 268,\n",
       " 39947,\n",
       " 3,\n",
       " 6531,\n",
       " 1343,\n",
       " 105,\n",
       " 153,\n",
       " 4,\n",
       " 19533,\n",
       " 8962,\n",
       " 863,\n",
       " 107,\n",
       " 1445,\n",
       " 75,\n",
       " 3,\n",
       " 580,\n",
       " 3005,\n",
       " 139,\n",
       " 1596,\n",
       " 649,\n",
       " 39,\n",
       " 477,\n",
       " 1264,\n",
       " 372,\n",
       " 429,\n",
       " 6723,\n",
       " 351,\n",
       " 75,\n",
       " 4979,\n",
       " 139,\n",
       " 649,\n",
       " 39,\n",
       " 662,\n",
       " 55,\n",
       " 357,\n",
       " 656,\n",
       " 6,\n",
       " 416,\n",
       " 3483,\n",
       " 128,\n",
       " 114,\n",
       " 660,\n",
       " 797,\n",
       " 36,\n",
       " 69,\n",
       " 3232,\n",
       " 660,\n",
       " 797,\n",
       " 31,\n",
       " 281,\n",
       " 43,\n",
       " 3330,\n",
       " 905,\n",
       " 585,\n",
       " 50,\n",
       " 576,\n",
       " 3232,\n",
       " 8,\n",
       " 2,\n",
       " 3977,\n",
       " 63,\n",
       " 39,\n",
       " 195,\n",
       " 39948,\n",
       " 5414,\n",
       " 1395,\n",
       " 31,\n",
       " 281,\n",
       " 43,\n",
       " 6,\n",
       " 905,\n",
       " 53,\n",
       " 585,\n",
       " 50,\n",
       " 576,\n",
       " 3232,\n",
       " 1395,\n",
       " 34,\n",
       " 509,\n",
       " 5183,\n",
       " 1690,\n",
       " 2311,\n",
       " 940,\n",
       " 43,\n",
       " 85,\n",
       " 1805,\n",
       " 259,\n",
       " 31,\n",
       " 743,\n",
       " 563,\n",
       " 122,\n",
       " 254,\n",
       " 50,\n",
       " 259,\n",
       " 613,\n",
       " 31,\n",
       " 235,\n",
       " 215,\n",
       " 8528,\n",
       " 10017,\n",
       " 43,\n",
       " 34,\n",
       " 155,\n",
       " 1610,\n",
       " 417,\n",
       " 39,\n",
       " 434,\n",
       " 613,\n",
       " 188,\n",
       " 1165,\n",
       " 1185,\n",
       " 2843,\n",
       " 3029,\n",
       " 535,\n",
       " 2205,\n",
       " 1610,\n",
       " 235,\n",
       " 3586,\n",
       " 126,\n",
       " 61,\n",
       " 393,\n",
       " 951,\n",
       " 31,\n",
       " 3,\n",
       " 43,\n",
       " 286,\n",
       " 2515,\n",
       " 3228,\n",
       " 728,\n",
       " 1140,\n",
       " 7206,\n",
       " 4860,\n",
       " 1815,\n",
       " 129,\n",
       " 1039,\n",
       " 4861,\n",
       " 6532,\n",
       " 161,\n",
       " 662,\n",
       " 55,\n",
       " 357,\n",
       " 13955,\n",
       " 11517,\n",
       " 6,\n",
       " 710,\n",
       " 128,\n",
       " 114,\n",
       " 660,\n",
       " 585,\n",
       " 1691,\n",
       " 50,\n",
       " 797,\n",
       " 576,\n",
       " 36,\n",
       " 610,\n",
       " 66,\n",
       " 55,\n",
       " 31,\n",
       " 235,\n",
       " 417,\n",
       " 39,\n",
       " 434,\n",
       " 563,\n",
       " 4509,\n",
       " 1343,\n",
       " 779,\n",
       " 1520,\n",
       " 66,\n",
       " 55,\n",
       " 215,\n",
       " 15898,\n",
       " 4336,\n",
       " 731,\n",
       " 39949,\n",
       " 111,\n",
       " 557,\n",
       " 3030,\n",
       " 2161,\n",
       " 53,\n",
       " 259,\n",
       " 605,\n",
       " 192,\n",
       " 147,\n",
       " 1741,\n",
       " 66,\n",
       " 55,\n",
       " 31,\n",
       " 235,\n",
       " 281,\n",
       " 39950,\n",
       " 11518,\n",
       " 1877,\n",
       " 826,\n",
       " 36,\n",
       " 335,\n",
       " 61,\n",
       " 97,\n",
       " 66,\n",
       " 55,\n",
       " 31,\n",
       " 563,\n",
       " 281,\n",
       " 2,\n",
       " 189,\n",
       " 3,\n",
       " 4337,\n",
       " 430,\n",
       " 118,\n",
       " 1302,\n",
       " 901,\n",
       " 206,\n",
       " 7824,\n",
       " 7825,\n",
       " 118,\n",
       " 1035,\n",
       " 654,\n",
       " 60,\n",
       " 451,\n",
       " 3858,\n",
       " 98,\n",
       " 600,\n",
       " 48,\n",
       " 56,\n",
       " 6188,\n",
       " 1949,\n",
       " 391,\n",
       " 1402,\n",
       " 358,\n",
       " 39951,\n",
       " 1312,\n",
       " 8,\n",
       " 2,\n",
       " 1026,\n",
       " 739,\n",
       " 563,\n",
       " 1637,\n",
       " 2684,\n",
       " 2375,\n",
       " 20,\n",
       " 10692,\n",
       " 1312,\n",
       " 549,\n",
       " 3007,\n",
       " 1282,\n",
       " 1249,\n",
       " 2260,\n",
       " 50,\n",
       " 1070,\n",
       " 663,\n",
       " 103,\n",
       " 34,\n",
       " 43,\n",
       " 968,\n",
       " 103,\n",
       " 177,\n",
       " 3,\n",
       " 869,\n",
       " 814,\n",
       " 639,\n",
       " 2708,\n",
       " 2784,\n",
       " 2283,\n",
       " 3859,\n",
       " 7498,\n",
       " 2466,\n",
       " 654,\n",
       " 391,\n",
       " 1302,\n",
       " 901,\n",
       " 61,\n",
       " 297,\n",
       " 1402,\n",
       " 259,\n",
       " 159,\n",
       " 31,\n",
       " 947,\n",
       " 940,\n",
       " 201,\n",
       " 620,\n",
       " 312,\n",
       " 43,\n",
       " 1424,\n",
       " 461,\n",
       " 364,\n",
       " 657,\n",
       " 1193,\n",
       " 1181,\n",
       " 187,\n",
       " 113,\n",
       " 586,\n",
       " 1728,\n",
       " 1766,\n",
       " 1729,\n",
       " 280,\n",
       " 1610,\n",
       " 20,\n",
       " 15899,\n",
       " 681,\n",
       " 477,\n",
       " 3676,\n",
       " 711,\n",
       " 2312,\n",
       " 440,\n",
       " 2807,\n",
       " 147,\n",
       " 2636,\n",
       " 25229,\n",
       " 13,\n",
       " 98,\n",
       " 13956,\n",
       " 1151,\n",
       " 1741,\n",
       " 8529,\n",
       " 66,\n",
       " 55,\n",
       " 2376,\n",
       " 438,\n",
       " 20,\n",
       " 2909,\n",
       " 256,\n",
       " 2124,\n",
       " 991,\n",
       " 1638,\n",
       " 11519,\n",
       " 2226,\n",
       " 1336,\n",
       " 39952,\n",
       " 7499,\n",
       " 4672,\n",
       " 1568,\n",
       " 25230,\n",
       " 2105,\n",
       " 1537,\n",
       " 39953,\n",
       " 3587,\n",
       " 12542,\n",
       " 2437,\n",
       " 739,\n",
       " 348,\n",
       " 113,\n",
       " 3,\n",
       " 1027,\n",
       " 2844,\n",
       " 93,\n",
       " 1312,\n",
       " 2467,\n",
       " 6023,\n",
       " 2413,\n",
       " 3082,\n",
       " 61,\n",
       " 3266,\n",
       " 881,\n",
       " 201,\n",
       " 3632,\n",
       " 2537,\n",
       " 1283,\n",
       " 569,\n",
       " 1861,\n",
       " 1255,\n",
       " 151,\n",
       " 1323,\n",
       " 6189,\n",
       " 39954,\n",
       " 4031,\n",
       " 2003,\n",
       " 36,\n",
       " 948,\n",
       " 66,\n",
       " 55,\n",
       " 177,\n",
       " 3,\n",
       " 379,\n",
       " 189,\n",
       " 3,\n",
       " 546,\n",
       " 325,\n",
       " 725,\n",
       " 835,\n",
       " 1742,\n",
       " 1060,\n",
       " 1669,\n",
       " 794,\n",
       " 114,\n",
       " 2173,\n",
       " 1549,\n",
       " 7826,\n",
       " 8,\n",
       " 2,\n",
       " 39955,\n",
       " 1384,\n",
       " 1965,\n",
       " 782,\n",
       " 25231,\n",
       " 39956,\n",
       " 698,\n",
       " 621,\n",
       " 961,\n",
       " 681,\n",
       " 10018,\n",
       " 7207,\n",
       " 1046,\n",
       " 1136,\n",
       " 3138,\n",
       " 843,\n",
       " 491,\n",
       " 1344,\n",
       " 1901,\n",
       " 1054,\n",
       " 641,\n",
       " 346,\n",
       " 5860,\n",
       " 441,\n",
       " 441,\n",
       " 2516,\n",
       " 605,\n",
       " 159,\n",
       " 947,\n",
       " 276,\n",
       " 854,\n",
       " 3917,\n",
       " 69,\n",
       " 66,\n",
       " 55,\n",
       " 909,\n",
       " 325,\n",
       " 312,\n",
       " 193,\n",
       " 109,\n",
       " 132,\n",
       " 489,\n",
       " 312,\n",
       " 139,\n",
       " 630,\n",
       " 369,\n",
       " 1385,\n",
       " 312,\n",
       " 39957,\n",
       " 1493,\n",
       " 1290,\n",
       " 13957,\n",
       " 3633,\n",
       " 369,\n",
       " 206,\n",
       " 393,\n",
       " 1902,\n",
       " 369,\n",
       " 854,\n",
       " 455,\n",
       " 676,\n",
       " 36,\n",
       " 69,\n",
       " 254,\n",
       " 321,\n",
       " 58,\n",
       " 485,\n",
       " 967,\n",
       " 881,\n",
       " 103,\n",
       " 377,\n",
       " 569,\n",
       " 2435,\n",
       " 732,\n",
       " 216,\n",
       " 491,\n",
       " 1436,\n",
       " 8154,\n",
       " 5184,\n",
       " 843,\n",
       " 356,\n",
       " 701,\n",
       " 259,\n",
       " 461,\n",
       " 20,\n",
       " 81,\n",
       " 1031,\n",
       " 66,\n",
       " 55,\n",
       " 441,\n",
       " 491,\n",
       " 375,\n",
       " 467,\n",
       " 395,\n",
       " 1370,\n",
       " 254,\n",
       " 13958,\n",
       " 2537,\n",
       " 1174,\n",
       " 1031,\n",
       " 1015,\n",
       " 356,\n",
       " 5861,\n",
       " 2491,\n",
       " 39958,\n",
       " 1862,\n",
       " 36,\n",
       " 69,\n",
       " 455,\n",
       " 440,\n",
       " 676,\n",
       " 155,\n",
       " 111,\n",
       " 3484,\n",
       " 59,\n",
       " 1356,\n",
       " 961,\n",
       " 1424,\n",
       " 461,\n",
       " 364,\n",
       " 1070,\n",
       " 43,\n",
       " 34,\n",
       " 3,\n",
       " 869,\n",
       " 397,\n",
       " 391,\n",
       " 173,\n",
       " 2492,\n",
       " 794,\n",
       " 114,\n",
       " 186,\n",
       " 1315,\n",
       " 1647,\n",
       " 179,\n",
       " 660,\n",
       " 7208,\n",
       " 3485,\n",
       " 2227,\n",
       " 2654,\n",
       " 36,\n",
       " 69,\n",
       " 34,\n",
       " 779,\n",
       " 39959,\n",
       " 676,\n",
       " 66,\n",
       " 55,\n",
       " 215,\n",
       " 533,\n",
       " 455,\n",
       " 440,\n",
       " 111,\n",
       " 59,\n",
       " 50,\n",
       " 36,\n",
       " 1204,\n",
       " 469,\n",
       " 8,\n",
       " 2,\n",
       " 1046,\n",
       " 39960,\n",
       " 20,\n",
       " 2206,\n",
       " 4083,\n",
       " 3233,\n",
       " 126,\n",
       " 528,\n",
       " 2414,\n",
       " 703,\n",
       " 3860,\n",
       " 748,\n",
       " 567,\n",
       " 4138,\n",
       " 749,\n",
       " 881,\n",
       " 377,\n",
       " 1115,\n",
       " 573,\n",
       " 457,\n",
       " 2435,\n",
       " 294,\n",
       " 306,\n",
       " 173,\n",
       " 4590,\n",
       " 1249,\n",
       " 189,\n",
       " 3,\n",
       " 109,\n",
       " 4337,\n",
       " 430,\n",
       " 794,\n",
       " 114,\n",
       " 1402,\n",
       " 11520,\n",
       " 503,\n",
       " 731,\n",
       " 130,\n",
       " 577,\n",
       " 1057,\n",
       " 126,\n",
       " 546,\n",
       " 325,\n",
       " 725,\n",
       " 835,\n",
       " 1742,\n",
       " 4769,\n",
       " 910,\n",
       " 346,\n",
       " 215,\n",
       " 1363,\n",
       " 254,\n",
       " 1903,\n",
       " 276,\n",
       " 1109,\n",
       " 593,\n",
       " 346,\n",
       " 1538,\n",
       " 605,\n",
       " 999,\n",
       " 621,\n",
       " 992,\n",
       " 586,\n",
       " 8155,\n",
       " 13959,\n",
       " 332,\n",
       " 66,\n",
       " 55,\n",
       " 294,\n",
       " 621,\n",
       " 13,\n",
       " 405,\n",
       " 66,\n",
       " 55,\n",
       " 2493,\n",
       " 2494,\n",
       " 294,\n",
       " 1224,\n",
       " 1816,\n",
       " 179,\n",
       " 582,\n",
       " 332,\n",
       " 103,\n",
       " 1210,\n",
       " 375,\n",
       " 173,\n",
       " 1836,\n",
       " 635,\n",
       " 568,\n",
       " 15900,\n",
       " 5706,\n",
       " 558,\n",
       " 189,\n",
       " 3,\n",
       " 794,\n",
       " 114,\n",
       " 1009,\n",
       " 1039,\n",
       " 3634,\n",
       " 105,\n",
       " 557,\n",
       " 993,\n",
       " 1094,\n",
       " 4338,\n",
       " 50,\n",
       " 39961,\n",
       " 763,\n",
       " 1977,\n",
       " 215,\n",
       " 586,\n",
       " 5287,\n",
       " 6359,\n",
       " 683,\n",
       " 2614,\n",
       " 296,\n",
       " 499,\n",
       " 7827,\n",
       " 1730,\n",
       " 193,\n",
       " 1878,\n",
       " 6360,\n",
       " 2637,\n",
       " 7209,\n",
       " 8530,\n",
       " 1402,\n",
       " 1718,\n",
       " 33,\n",
       " 36,\n",
       " 1676,\n",
       " 1345,\n",
       " 166,\n",
       " 404,\n",
       " 47,\n",
       " 4139,\n",
       " 1403,\n",
       " 1696,\n",
       " 5707,\n",
       " 1826,\n",
       " 1788,\n",
       " 1657,\n",
       " 8531,\n",
       " 15901,\n",
       " 66,\n",
       " 55,\n",
       " 1836,\n",
       " 427,\n",
       " 8,\n",
       " 2,\n",
       " 1848,\n",
       " 61,\n",
       " 4434,\n",
       " 383,\n",
       " 2537,\n",
       " 4,\n",
       " 522,\n",
       " 2738,\n",
       " 2594,\n",
       " 654,\n",
       " 358,\n",
       " 1141,\n",
       " 104,\n",
       " 189,\n",
       " 3,\n",
       " 1719,\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bachelet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e177d02bbc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bachelet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'final'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'orrego'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'bachelet'"
     ]
    }
   ],
   "source": [
    "for word in ['bachelet', 'final', 'orrego']:\n",
    "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las secuencias generadas con texts_to_sequences no poseen un largo uniforme, se utiliza pad_sequence para remediar dicho resultado mediante la adición de ceros a las secuencias hasta homogeneizar el largo de estas últimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4087"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(len(t) for t in X_train_sequences)\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_sequences = pad_sequences(X_train_sequences, padding='post', maxlen=max_sequence_length)\n",
    "X_test_padded_sequences = pad_sequences(X_test_sequences, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1343, 4087)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1394, 1254, 1078, ...,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding Layer\n",
    "\n",
    "+ [Artículo relevante](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n",
    "+ [Documentación embedding layer](https://keras.io/layers/embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4087, 50)          4830700   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 204350)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                13078464  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 17,911,504\n",
      "Trainable params: 17,911,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=max_sequence_length))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(len(df['Type'].unique()), activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_4 to have shape (36,) but got array with shape (31,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c56bf8e8f83b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_padded_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 batch_size=batch_size)\n\u001b[0m\u001b[1;32m    973\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0mval_ins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_4 to have shape (36,) but got array with shape (31,)"
     ]
    }
   ],
   "source": [
    "model_name = 'ffn'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9938\n",
      "Testing Accuracy:  0.5979\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Preentrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Características del embedding: \n",
    "+ #dimensions = 300\n",
    "+ #vectors = 1000653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = '../embeddings/SBW-vectors-300-min5.txt'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabra dentro del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.96480e-02,  1.13360e-02,  1.99490e-02, -8.88320e-02,\n",
       "       -2.52250e-02,  5.68440e-02,  2.54730e-02,  1.40680e-02,\n",
       "        1.63694e-01, -6.71540e-02,  1.47380e-02,  2.71340e-02,\n",
       "        6.64430e-02, -4.48460e-02, -4.49870e-02, -4.08980e-02,\n",
       "        3.03110e-02,  3.41960e-02, -4.92400e-02,  8.53700e-03,\n",
       "       -6.80910e-02, -8.79380e-02,  3.53000e-02,  1.49385e-01,\n",
       "       -1.23500e-02,  1.26130e-02,  2.93500e-02,  6.95960e-02,\n",
       "        3.91110e-02,  5.76520e-02,  6.99540e-02, -6.62170e-02,\n",
       "       -4.17840e-02,  2.86230e-02,  2.67720e-02, -6.63920e-02,\n",
       "        2.95300e-03, -1.21880e-02, -3.03630e-02,  4.02220e-02,\n",
       "        3.48580e-02,  2.74690e-02, -2.90340e-02, -4.87480e-02,\n",
       "       -3.85820e-02, -5.15530e-02, -3.35010e-02, -1.90080e-02,\n",
       "        3.04300e-03,  1.10712e-01, -2.50960e-02,  1.11082e-01,\n",
       "        3.52440e-02,  1.14207e-01,  1.01950e-02,  5.15110e-02,\n",
       "       -4.06490e-02, -1.13944e-01,  4.48730e-02,  5.20110e-02,\n",
       "        6.73600e-02,  4.90540e-02, -1.27085e-01, -3.18460e-02,\n",
       "        3.28480e-02,  4.08250e-02, -8.48730e-02,  5.98010e-02,\n",
       "       -6.74240e-02,  1.65310e-02, -8.45650e-02,  5.70240e-02,\n",
       "        8.32880e-02, -1.01360e-02, -4.85080e-02,  5.17570e-02,\n",
       "        4.66640e-02,  1.81020e-02, -5.23200e-02, -7.65000e-04,\n",
       "        5.36620e-02, -9.96700e-03,  8.28580e-02,  9.06800e-03,\n",
       "        5.45750e-02, -3.46600e-03, -2.33760e-02,  2.30690e-02,\n",
       "        8.85130e-02,  1.85040e-02, -3.95030e-02, -3.29800e-02,\n",
       "       -2.13900e-03,  1.00000e-05, -1.07627e-01,  7.69900e-03,\n",
       "        4.63510e-02, -3.06200e-03,  3.05000e-02,  1.13650e-01,\n",
       "        3.25360e-02, -9.73010e-02, -1.37340e-02,  9.83450e-02,\n",
       "        8.08980e-02, -6.41730e-02, -8.87400e-03, -1.44751e-01,\n",
       "        3.75850e-02,  1.32900e-02,  5.96740e-02,  6.16300e-03,\n",
       "        7.31800e-03,  5.30000e-05, -6.02920e-02, -5.91350e-02,\n",
       "        4.94970e-02, -1.14380e-02, -9.51080e-02, -4.34650e-02,\n",
       "        4.85670e-02, -4.39900e-02, -3.07740e-02,  5.09200e-03,\n",
       "       -3.22650e-02,  9.39200e-03,  1.85030e-02,  8.48570e-02,\n",
       "        1.09709e-01, -2.06620e-02,  1.76960e-02,  2.66990e-02,\n",
       "       -7.66380e-02, -1.41060e-02, -3.51550e-02,  4.69990e-02,\n",
       "       -3.72700e-03, -4.78050e-02,  4.42700e-02,  1.13140e-02,\n",
       "        3.65240e-02, -6.95050e-02, -1.48500e-02, -3.53800e-03,\n",
       "       -4.70490e-02,  2.93490e-02,  3.45210e-02, -3.21990e-02,\n",
       "        1.16497e-01, -7.76100e-02,  6.82340e-02, -1.61260e-02,\n",
       "       -6.64540e-02, -7.99140e-02, -2.07230e-02, -6.49050e-02,\n",
       "        6.95600e-02,  2.13680e-02, -4.94970e-02, -4.65990e-02,\n",
       "        6.76630e-02, -6.90350e-02,  1.18015e-01,  2.74630e-02,\n",
       "       -6.17600e-03, -3.45140e-02, -2.65150e-02,  4.03080e-02,\n",
       "        9.11130e-02, -8.05390e-02,  1.32408e-01, -7.09580e-02,\n",
       "        1.97300e-02,  3.33990e-02,  3.48900e-03, -1.59659e-01,\n",
       "       -4.23000e-03,  4.88800e-03, -5.66150e-02,  6.10210e-02,\n",
       "        2.51170e-02,  9.96130e-02,  6.38760e-02, -6.20200e-03,\n",
       "       -4.93160e-02, -2.05300e-02,  8.52200e-03, -9.41680e-02,\n",
       "       -9.45100e-03, -3.46820e-02, -2.68010e-02,  6.53830e-02,\n",
       "        4.25280e-02, -1.36880e-02,  3.50680e-02,  1.19803e-01,\n",
       "       -2.02780e-02,  1.11882e-01, -6.83360e-02,  5.02450e-02,\n",
       "       -1.23240e-01,  2.24800e-03,  1.02290e-02, -6.25400e-02,\n",
       "       -1.78370e-02, -1.15210e-01,  5.10360e-02,  2.69200e-02,\n",
       "        8.34570e-02,  6.29020e-02,  3.08300e-03, -3.71120e-02,\n",
       "       -1.54250e-02,  1.71600e-03,  1.80020e-02,  1.01100e-01,\n",
       "        1.94460e-02,  7.48390e-02,  5.99510e-02,  7.22430e-02,\n",
       "       -2.54590e-02, -3.98530e-02,  7.79500e-02,  6.01990e-02,\n",
       "       -7.06030e-02,  6.06520e-02,  2.38550e-02, -3.58580e-02,\n",
       "       -5.80430e-02, -7.51300e-02,  6.70000e-04,  8.28280e-02,\n",
       "       -1.31410e-02,  1.44692e-01, -1.05775e-01,  8.76240e-02,\n",
       "        4.70300e-02, -1.57000e-02, -4.23600e-02,  2.63370e-02,\n",
       "        1.44966e-01,  2.19220e-02,  1.20970e-02, -1.14430e-02,\n",
       "       -1.10208e-01,  9.33300e-03, -8.14230e-02, -2.71370e-02,\n",
       "        8.27000e-04,  8.53890e-02,  3.44640e-02,  3.23380e-02,\n",
       "       -1.50400e-02,  9.95400e-03, -1.17590e-02, -4.23950e-02,\n",
       "       -4.67000e-04,  5.50470e-02, -4.97100e-02, -1.04978e-01,\n",
       "        3.17420e-02,  3.70200e-02, -7.01600e-03, -4.97120e-02,\n",
       "        4.16840e-02,  1.83480e-02, -1.20100e-03,  1.92820e-02,\n",
       "        5.76300e-03,  7.49520e-02, -1.85880e-02,  1.60550e-02,\n",
       "        1.19526e-01, -1.46130e-02,  2.05980e-02,  2.73120e-02,\n",
       "        2.42520e-02, -2.40440e-02, -2.63320e-02, -6.31520e-02,\n",
       "       -9.53630e-02, -3.43350e-02, -6.25520e-02, -3.57300e-02,\n",
       "        9.11650e-02,  9.68600e-03,  2.03410e-02, -1.20040e-02,\n",
       "        1.18260e-02, -8.43010e-02,  1.11440e-02,  7.49690e-02,\n",
       "       -4.86200e-03, -1.40760e-02,  2.56920e-02, -7.73220e-02,\n",
       "       -2.29980e-02, -1.28057e-01, -4.91700e-03,  6.26280e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors['de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29014, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los vectores para el vocabulario del corpus de entrenamiento, desde el modelo word2vect preentrenado. Si no se encuentra el vector para alguna palabra (Out of Vocabulary Word), se genera uno aleatorio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = wordvectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29014, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN + word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 173400)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                11097664  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 19,802,124\n",
      "Trainable params: 19,802,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "                    input_dim=vocab_size, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=max_sequence_length,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True\n",
    "                )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 1.0203 - acc: 0.5294 - val_loss: 0.8968 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61499, saving model to ../model_checkpoints/polarity/ffn-w2v-2019-06-04T11:33:18.613130.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 0.4494 - acc: 0.8373 - val_loss: 0.9868 - val_acc: 0.6105\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.61499\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 0.1099 - acc: 0.9679 - val_loss: 1.2182 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61499\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 84s 7ms/step - loss: 0.0543 - acc: 0.9859 - val_loss: 1.2905 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61499\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 83s 7ms/step - loss: 0.0387 - acc: 0.9897 - val_loss: 1.4088 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61499\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ffn-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_padded_sequences)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN - Yoon Kim Model + word2vec\n",
    "\n",
    "[model reference](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 574, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 191, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 187, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 33, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 9,077,452\n",
      "Trainable params: 9,077,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=5),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 1.0058 - acc: 0.5385 - val_loss: 0.9255 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59941, saving model to ../model_checkpoints/polarity/cnn-w2v-2019-06-04T11:42:59.698089.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 178s 15ms/step - loss: 0.6332 - acc: 0.7536 - val_loss: 0.9545 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59941 to 0.61721, saving model to ../model_checkpoints/polarity/cnn-w2v-2019-06-04T11:42:59.698089.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 178s 15ms/step - loss: 0.2576 - acc: 0.9130 - val_loss: 1.4142 - val_acc: 0.5638\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61721\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 0.1268 - acc: 0.9594 - val_loss: 1.5104 - val_acc: 0.6142\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61721\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 0.0644 - acc: 0.9800 - val_loss: 1.8126 - val_acc: 0.5816\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61721\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 176s 15ms/step - loss: 0.0402 - acc: 0.9855 - val_loss: 1.9659 - val_acc: 0.5675\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61721\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9917\n",
      "Testing Accuracy:  0.5675\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Yoon Kim model (padding = 'same') + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 578, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 192, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 64, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,984,268\n",
      "Trainable params: 8,984,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length, \n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 136s 11ms/step - loss: 1.0030 - acc: 0.5354 - val_loss: 0.9029 - val_acc: 0.6313\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63131, saving model to ../model_checkpoints/polarity/cnn-w2v-padding-same-2019-06-04T12:06:03.234815.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 153s 13ms/step - loss: 0.6074 - acc: 0.7634 - val_loss: 0.9282 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.63131\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 137s 11ms/step - loss: 0.2275 - acc: 0.9194 - val_loss: 1.2543 - val_acc: 0.5883\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.63131\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.0966 - acc: 0.9689 - val_loss: 1.7408 - val_acc: 0.5757\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63131\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 135s 11ms/step - loss: 0.0504 - acc: 0.9833 - val_loss: 1.9429 - val_acc: 0.5801\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.63131\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-w2v-padding-same'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Yoon Kim model + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = KeyedVectors.load_word2vec_format('../embeddings/glove-sbwc.i25.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = glove_vectors[word]\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        glove_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(glove_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 576, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 189, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 59, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,984,268\n",
      "Trainable params: 8,984,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 130s 11ms/step - loss: 0.9480 - acc: 0.5785 - val_loss: 0.8520 - val_acc: 0.6476\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64763, saving model to ../model_checkpoints/polarity/cnn-glove-2019-06-04T12:25:03.286500.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 132s 11ms/step - loss: 0.6512 - acc: 0.7405 - val_loss: 0.8910 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64763\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 132s 11ms/step - loss: 0.3063 - acc: 0.8910 - val_loss: 1.0914 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64763\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 138s 11ms/step - loss: 0.1325 - acc: 0.9568 - val_loss: 1.5486 - val_acc: 0.5920\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64763\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 138s 11ms/step - loss: 0.0641 - acc: 0.9814 - val_loss: 1.5703 - val_acc: 0.6187\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64763\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9898\n",
      "Testing Accuracy:  0.6187\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (padding 'same') + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 578, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 192, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 64, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,984,268\n",
      "Trainable params: 8,984,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 132s 11ms/step - loss: 0.9345 - acc: 0.5820 - val_loss: 0.8761 - val_acc: 0.6372\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63724, saving model to ../model_checkpoints/polarity/cnn-glove-padding-same-2019-06-04T12:41:03.316601.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 133s 11ms/step - loss: 0.6315 - acc: 0.7498 - val_loss: 0.9105 - val_acc: 0.6447\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63724 to 0.64466, saving model to ../model_checkpoints/polarity/cnn-glove-padding-same-2019-06-04T12:41:03.316601.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.2857 - acc: 0.8944 - val_loss: 1.1604 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64466\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.1212 - acc: 0.9617 - val_loss: 1.4062 - val_acc: 0.6135\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64466\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 134s 11ms/step - loss: 0.0555 - acc: 0.9851 - val_loss: 1.7002 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64466\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 133s 11ms/step - loss: 0.0334 - acc: 0.9894 - val_loss: 1.9989 - val_acc: 0.6165\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.64466\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cnn-glove-padding-same'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          8346900   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 8,507,603\n",
      "Trainable params: 8,507,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11192 samples, validate on 1244 samples\n",
      "Epoch 1/5\n",
      "11192/11192 [==============================] - 216s 19ms/step - loss: 1.0517 - acc: 0.4335 - val_loss: 1.0553 - val_acc: 0.4341\n",
      "Epoch 2/5\n",
      "11192/11192 [==============================] - 225s 20ms/step - loss: 1.0507 - acc: 0.4372 - val_loss: 1.0546 - val_acc: 0.4341\n",
      "Epoch 3/5\n",
      "11192/11192 [==============================] - 231s 21ms/step - loss: 1.0505 - acc: 0.4336 - val_loss: 1.0543 - val_acc: 0.4341\n",
      "Epoch 4/5\n",
      "11192/11192 [==============================] - 219s 20ms/step - loss: 1.0503 - acc: 0.4372 - val_loss: 1.0547 - val_acc: 0.4341\n",
      "Epoch 5/5\n",
      "11192/11192 [==============================] - 212s 19ms/step - loss: 1.0497 - acc: 0.4372 - val_loss: 1.0557 - val_acc: 0.4341\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,878,503\n",
      "Trainable params: 7,878,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 159s 16ms/step - loss: 1.0539 - acc: 0.4331 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0503 - acc: 0.4316 - val_loss: 1.0498 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 165s 17ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0522 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 168s 17ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0489 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 173s 17ms/step - loss: 1.0508 - acc: 0.4344 - val_loss: 1.0492 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4344\n",
      "Testing Accuracy:  0.4469\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 578, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,891,596\n",
      "Trainable params: 8,891,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 331s 27ms/step - loss: 0.9755 - acc: 0.5545 - val_loss: 0.8582 - val_acc: 0.6395\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63947, saving model to ../model_checkpoints/polarity/biLSTM-2019-06-04T12:57:07.503644.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 330s 27ms/step - loss: 0.5536 - acc: 0.7959 - val_loss: 0.9465 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.63947\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 335s 28ms/step - loss: 0.2168 - acc: 0.9267 - val_loss: 1.2999 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.63947\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 344s 28ms/step - loss: 0.0982 - acc: 0.9704 - val_loss: 1.4427 - val_acc: 0.5950\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.63947\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 330s 27ms/step - loss: 0.0585 - acc: 0.9824 - val_loss: 1.6469 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.63947\n"
     ]
    }
   ],
   "source": [
    "model_name = 'biLSTM'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 578, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,891,596\n",
      "Trainable params: 8,891,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 330s 27ms/step - loss: 0.9832 - acc: 0.5573 - val_loss: 0.8543 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64318, saving model to ../model_checkpoints/polarity/bilstm-w2v-2019-06-04T13:26:10.587398.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 334s 28ms/step - loss: 0.6020 - acc: 0.7679 - val_loss: 0.8965 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64318\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 337s 28ms/step - loss: 0.2680 - acc: 0.9094 - val_loss: 1.1728 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64318\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 341s 28ms/step - loss: 0.1176 - acc: 0.9626 - val_loss: 1.4839 - val_acc: 0.5875\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64318\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 335s 28ms/step - loss: 0.0672 - acc: 0.9795 - val_loss: 1.7182 - val_acc: 0.5801\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64318\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bilstm-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 578, 128)          186880    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,891,596\n",
      "Trainable params: 8,891,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 344s 28ms/step - loss: 0.9163 - acc: 0.5970 - val_loss: 0.8276 - val_acc: 0.6491\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64911, saving model to ../model_checkpoints/polarity/bilstm-glove-2019-06-04T13:57:37.018938.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 350s 29ms/step - loss: 0.6463 - acc: 0.7444 - val_loss: 0.8090 - val_acc: 0.6677\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64911 to 0.66766, saving model to ../model_checkpoints/polarity/bilstm-glove-2019-06-04T13:57:37.018938.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 352s 29ms/step - loss: 0.3463 - acc: 0.8744 - val_loss: 0.9961 - val_acc: 0.6521\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.66766\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 342s 28ms/step - loss: 0.1512 - acc: 0.9511 - val_loss: 1.2563 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.66766\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 338s 28ms/step - loss: 0.0729 - acc: 0.9782 - val_loss: 1.4715 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.66766\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 337s 28ms/step - loss: 0.0477 - acc: 0.9846 - val_loss: 1.6747 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.66766\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bilstm-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               120300    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,838,403\n",
      "Trainable params: 7,838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length),\n",
    "    layers.GRU(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 130s 13ms/step - loss: 1.0530 - acc: 0.4282 - val_loss: 1.0500 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 132s 13ms/step - loss: 1.0518 - acc: 0.4289 - val_loss: 1.0511 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 126s 13ms/step - loss: 1.0506 - acc: 0.4306 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 133s 13ms/step - loss: 1.0505 - acc: 0.4344 - val_loss: 1.0515 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 130s 13ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0520 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 578, 300)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 100)               120300    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,838,403\n",
      "Trainable params: 7,838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.SpatialDropout1D(0.2),\n",
    "    layers.GRU(64),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0513 - acc: 0.4335 - val_loss: 1.0488 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 156s 16ms/step - loss: 1.0521 - acc: 0.4344 - val_loss: 1.0512 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 153s 15ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0586 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 169s 17ms/step - loss: 1.0506 - acc: 0.4344 - val_loss: 1.0507 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0503 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,844,876\n",
      "Trainable params: 8,844,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.9822 - acc: 0.5555 - val_loss: 0.8561 - val_acc: 0.6335\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63353, saving model to ../model_checkpoints/polarity/bigru-w2v-2019-06-04T14:32:32.358771.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.5851 - acc: 0.7739 - val_loss: 0.8931 - val_acc: 0.6424\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63353 to 0.64243, saving model to ../model_checkpoints/polarity/bigru-w2v-2019-06-04T14:32:32.358771.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 276s 23ms/step - loss: 0.2375 - acc: 0.9180 - val_loss: 1.1556 - val_acc: 0.6172\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64243\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.0989 - acc: 0.9701 - val_loss: 1.4688 - val_acc: 0.5861\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64243\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 277s 23ms/step - loss: 0.0564 - acc: 0.9835 - val_loss: 1.6570 - val_acc: 0.5972\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64243\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.0374 - acc: 0.9886 - val_loss: 1.8158 - val_acc: 0.5905\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.64243\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 8,844,876\n",
      "Trainable params: 8,844,876\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.GlobalMaxPool1D(),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.9129 - acc: 0.6033 - val_loss: 0.8218 - val_acc: 0.6484\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64837, saving model to ../model_checkpoints/polarity/bigru-glove-2019-06-04T15:49:00.634142.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.6249 - acc: 0.7516 - val_loss: 0.8350 - val_acc: 0.6432\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.64837\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 293s 24ms/step - loss: 0.3197 - acc: 0.8912 - val_loss: 0.9704 - val_acc: 0.6150\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.64837\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.1303 - acc: 0.9611 - val_loss: 1.2708 - val_acc: 0.6076\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.64837\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 279s 23ms/step - loss: 0.0678 - acc: 0.9813 - val_loss: 1.4299 - val_acc: 0.6046\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.64837\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN +  LSTM + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 574, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 143, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                3264      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 7,840,323\n",
      "Trainable params: 7,840,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 106s 11ms/step - loss: 1.0538 - acc: 0.4319 - val_loss: 1.0491 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 108s 11ms/step - loss: 1.0502 - acc: 0.4344 - val_loss: 1.0522 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 108s 11ms/step - loss: 1.0506 - acc: 0.4344 - val_loss: 1.0500 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 108s 11ms/step - loss: 1.0501 - acc: 0.4344 - val_loss: 1.0490 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 107s 11ms/step - loss: 1.0505 - acc: 0.4325 - val_loss: 1.0492 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + CNN + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 574, 64)           41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 8,889,804\n",
      "Trainable params: 8,889,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 308s 25ms/step - loss: 0.9646 - acc: 0.5644 - val_loss: 0.8728 - val_acc: 0.6180\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61795, saving model to ../model_checkpoints/polarity/bigru-cnn-w2v-2019-06-04T16:15:32.057807.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 309s 25ms/step - loss: 0.5510 - acc: 0.7898 - val_loss: 0.9550 - val_acc: 0.6194\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61795 to 0.61944, saving model to ../model_checkpoints/polarity/bigru-cnn-w2v-2019-06-04T16:15:32.057807.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 309s 25ms/step - loss: 0.2001 - acc: 0.9302 - val_loss: 1.3415 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.61944\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 311s 26ms/step - loss: 0.0870 - acc: 0.9714 - val_loss: 1.6379 - val_acc: 0.6001\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61944\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 321s 26ms/step - loss: 0.0529 - acc: 0.9820 - val_loss: 1.8591 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.61944\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 320s 26ms/step - loss: 0.0355 - acc: 0.9866 - val_loss: 1.9790 - val_acc: 0.5764\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61944\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-cnn-w2v'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiGRU + CNN + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 578, 300)          8704200   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 578, 128)          140160    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 574, 64)           41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 8,889,804\n",
      "Trainable params: 8,889,804\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Bidirectional(layers.GRU(64, return_sequences=True)),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12127 samples, validate on 1348 samples\n",
      "Epoch 1/10\n",
      "12127/12127 [==============================] - 315s 26ms/step - loss: 0.9176 - acc: 0.5993 - val_loss: 0.8457 - val_acc: 0.6454\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.64540, saving model to ../model_checkpoints/polarity/bigru-cnn-glove-2019-06-04T16:48:58.751815.hdf5\n",
      "Epoch 2/10\n",
      "12127/12127 [==============================] - 311s 26ms/step - loss: 0.6409 - acc: 0.7435 - val_loss: 0.8538 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.64540 to 0.65059, saving model to ../model_checkpoints/polarity/bigru-cnn-glove-2019-06-04T16:48:58.751815.hdf5\n",
      "Epoch 3/10\n",
      "12127/12127 [==============================] - 311s 26ms/step - loss: 0.3164 - acc: 0.8862 - val_loss: 1.1413 - val_acc: 0.6157\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.65059\n",
      "Epoch 4/10\n",
      "12127/12127 [==============================] - 315s 26ms/step - loss: 0.1300 - acc: 0.9572 - val_loss: 1.4410 - val_acc: 0.6165\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.65059\n",
      "Epoch 5/10\n",
      "12127/12127 [==============================] - 313s 26ms/step - loss: 0.0690 - acc: 0.9786 - val_loss: 1.7066 - val_acc: 0.6120\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.65059\n",
      "Epoch 6/10\n",
      "12127/12127 [==============================] - 308s 25ms/step - loss: 0.0390 - acc: 0.9875 - val_loss: 2.0310 - val_acc: 0.6239\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.65059\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bigru-cnn-glove'\n",
    "filepath = CHECKPOINTS_PATH + model_name + '-{}.hdf5'.format(datetime.today().isoformat())\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "es = EarlyStopping(monitor='val_acc', baseline=0.61, patience=4)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_padded_sequences, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True,\n",
    "    validation_data=(X_test_padded_sequences, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint, es]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/polarity-extended.csv', sep=\";\")\n",
    "df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0                                Abogado de Michelle Bachelet otorgó asesoría jurídica a mujer que realizó la denuncia. https://t.co/lV5gnWcfmm   \n",
       "1                        RT @Alitop_: Faltan 635 dias para que se acabe esta pesadilla llamada Michelle Bachelet #CuentaRegresiva #ChaoBachelet   \n",
       "2  Michelle Bachelet está trotando para estar en forma. Michelle Bachelet está tratando de aprobar sus reformas chavo!! https://t.co/0QAX2Hu2Gh   \n",
       "3  RT @ElLibido: 2/15 Hace pocos días, los “amigos” de @derechatuitera masificaron imagen, sobre supuesto vino de Michelle Bachelet. https://t…   \n",
       "4       Alcalde de Pozo Almonte, José Fernando Muñoz junto a la Presidenta, Michelle Bachelet e Intendenta de Tarapacá. https://t.co/v1IxZ4D3aG   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faltan dias acabe pesadilla llamada michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hace pocos días amigos masificaron imagen supuesto vino michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                        abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia   \n",
       "1                                            faltan dias acabe pesadilla llamada michelle bachelet   \n",
       "2               michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo   \n",
       "3                        hace pocos días amigos masificaron imagen supuesto vino michelle bachelet   \n",
       "4  alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(text_preprocessing.normalize, no_tweet_hashtags=True, no_camel_case=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        993\n",
       "polarity    993\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.text.str.contains('bachelet')]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6016320474777448\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "455px",
    "width": "241px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "288.85px",
    "left": "205px",
    "right": "361px",
    "top": "177px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
