{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/polarity.csv', sep=\";\")\n",
    "df.polarity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        12627\n",
       "polarity    12627\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>5485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "polarity      \n",
       "-1        5485\n",
       " 0        4645\n",
       " 1        2497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['polarity']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(text_preprocessing.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faltan dias acabe pesadilla llamada michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hace pocos días amigos masificaron imagen supuesto vino michelle bachelet</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text  \\\n",
       "0                        abogado michelle bachelet otorgó asesoría jurídica mujer realizó denuncia   \n",
       "1                                            faltan dias acabe pesadilla llamada michelle bachelet   \n",
       "2               michelle bachelet trotando forma michelle bachelet tratando aprobar reformas chavo   \n",
       "3                        hace pocos días amigos masificaron imagen supuesto vino michelle bachelet   \n",
       "4  alcalde pozo almonte josé fernando muñoz junto presidenta michelle bachelet intendenta tarapacá   \n",
       "\n",
       "   polarity  \n",
       "0         1  \n",
       "1        -1  \n",
       "2         0  \n",
       "3        -1  \n",
       "4         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove texts with only one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda r: len(r.text.split()) > 1, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text        12436\n",
       "polarity    12436\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text.values\n",
    "y_train = train.polarity.values\n",
    "X_test = test.text.values\n",
    "y_test = test.polarity.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.610128617363344\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.60      0.87      0.71      1112\n",
      "           0       0.59      0.53      0.56       881\n",
      "           1       0.85      0.17      0.29       495\n",
      "\n",
      "    accuracy                           0.61      2488\n",
      "   macro avg       0.68      0.52      0.52      2488\n",
      "weighted avg       0.65      0.61      0.57      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                ('clf',\n",
       "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=42,\n",
       "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6089228295819936\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.59      0.89      0.71      1112\n",
      "           0       0.63      0.41      0.50       881\n",
      "           1       0.67      0.34      0.45       495\n",
      "\n",
      "    accuracy                           0.61      2488\n",
      "   macro avg       0.63      0.55      0.55      2488\n",
      "weighted avg       0.62      0.61      0.58      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=1, penalty='l2',\n",
       "                                    random_state=None, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5727491961414791\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.66      0.66      1112\n",
      "           0       0.50      0.54      0.52       881\n",
      "           1       0.51      0.43      0.47       495\n",
      "\n",
      "    accuracy                           0.57      2488\n",
      "   macro avg       0.56      0.54      0.55      2488\n",
      "weighted avg       0.57      0.57      0.57      2488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - solo positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_neutral_df = df[~(df.polarity == 0)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>5433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text\n",
       "polarity      \n",
       "-1        5433\n",
       " 1        2446"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_neutral_df.groupby(['polarity']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_neutral, test_no_neutral = train_test_split(no_neutral_df, test_size=0.2, random_state=42)\n",
    "X_train_no_neutral = train_no_neutral.text.values\n",
    "X_test_no_neutral = test_no_neutral.text.values\n",
    "y_train_no_neutral = train_no_neutral.polarity.values\n",
    "y_test_no_neutral = test_no_neutral.polarity.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_no_neutral = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb_no_neutral.fit(X_train_no_neutral, y_train_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_no_neutral = nb_no_neutral.predict(X_test_no_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7848984771573604\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred_no_neutral, y_test_no_neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.77      0.99      0.87      1106\n",
      "           1       0.93      0.30      0.45       470\n",
      "\n",
      "    accuracy                           0.78      1576\n",
      "   macro avg       0.85      0.65      0.66      1576\n",
      "weighted avg       0.82      0.78      0.74      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_no_neutral, y_pred_no_neutral))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5928456591639871\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(n_estimators=100)),\n",
    "])\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5928456591639871\n"
     ]
    }
   ],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer labels to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.get_dummies(train.polarity).values\n",
    "y_test = pd.get_dummies(test.polarity).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final nefasto gobierno bachelet vamos terminar comiéndonos mocos orrego'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[262, 826, 15, 3, 107, 473, 9910, 6360, 6361]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bachelet: 3\n",
      "final: 262\n",
      "orrego: 6361\n"
     ]
    }
   ],
   "source": [
    "for word in ['bachelet', 'final', 'orrego']:\n",
    "    print('{}: {}'.format(word, tokenizer.word_index[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que las secuencias generadas con texts_to_sequences no poseen un largo uniforme, se utiliza pad_sequence para remediar dicho resultado mediante la adición de ceros a las secuencias hasta homogeneizar el largo de estas últimas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "578"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = max(len(t) for t in X_train_sequences)\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded_sequences = pad_sequences(X_train_sequences, padding='post', maxlen=max_sequence_length)\n",
    "X_test_padded_sequences = pad_sequences(X_test_sequences, padding='post', maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9948, 578)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  530,   57,  128,   50,    1,   77, 9909,   23,  222,  195,\n",
       "         94,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_sequences[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeding Layer\n",
    "\n",
    "+ [Artículo relevante](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer)\n",
    "+ [Documentación embedding layer](https://keras.io/layers/embeddings/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 578, 50)           1286300   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28900)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                289010    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 1,575,343\n",
      "Trainable params: 1,575,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                           output_dim=embedding_dim, \n",
    "                           input_length=max_sequence_length))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 16s 2ms/step - loss: 1.0493 - acc: 0.4342 - val_loss: 1.0208 - val_acc: 0.4594\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.9468 - acc: 0.4869 - val_loss: 0.9254 - val_acc: 0.5253\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.6775 - acc: 0.5899 - val_loss: 0.9838 - val_acc: 0.5478\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.2871 - acc: 0.8836 - val_loss: 1.2208 - val_acc: 0.5788\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0923 - acc: 0.9743 - val_loss: 1.3492 - val_acc: 0.5816\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0501 - acc: 0.9864 - val_loss: 1.5124 - val_acc: 0.5832\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0374 - acc: 0.9892 - val_loss: 1.6588 - val_acc: 0.5727\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0270 - acc: 0.9917 - val_loss: 1.6921 - val_acc: 0.5792\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0240 - acc: 0.9919 - val_loss: 1.7628 - val_acc: 0.5719\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 15s 2ms/step - loss: 0.0213 - acc: 0.9928 - val_loss: 1.7496 - val_acc: 0.5727\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9953\n",
      "Testing Accuracy:  0.5856\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Preentrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Características del embedding: \n",
    "+ #dimensions = 300\n",
    "+ #vectors = 1000653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvectors_file_vec = '../embeddings/SBW-vectors-300-min5.txt'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabra dentro del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.96480e-02,  1.13360e-02,  1.99490e-02, -8.88320e-02,\n",
       "       -2.52250e-02,  5.68440e-02,  2.54730e-02,  1.40680e-02,\n",
       "        1.63694e-01, -6.71540e-02,  1.47380e-02,  2.71340e-02,\n",
       "        6.64430e-02, -4.48460e-02, -4.49870e-02, -4.08980e-02,\n",
       "        3.03110e-02,  3.41960e-02, -4.92400e-02,  8.53700e-03,\n",
       "       -6.80910e-02, -8.79380e-02,  3.53000e-02,  1.49385e-01,\n",
       "       -1.23500e-02,  1.26130e-02,  2.93500e-02,  6.95960e-02,\n",
       "        3.91110e-02,  5.76520e-02,  6.99540e-02, -6.62170e-02,\n",
       "       -4.17840e-02,  2.86230e-02,  2.67720e-02, -6.63920e-02,\n",
       "        2.95300e-03, -1.21880e-02, -3.03630e-02,  4.02220e-02,\n",
       "        3.48580e-02,  2.74690e-02, -2.90340e-02, -4.87480e-02,\n",
       "       -3.85820e-02, -5.15530e-02, -3.35010e-02, -1.90080e-02,\n",
       "        3.04300e-03,  1.10712e-01, -2.50960e-02,  1.11082e-01,\n",
       "        3.52440e-02,  1.14207e-01,  1.01950e-02,  5.15110e-02,\n",
       "       -4.06490e-02, -1.13944e-01,  4.48730e-02,  5.20110e-02,\n",
       "        6.73600e-02,  4.90540e-02, -1.27085e-01, -3.18460e-02,\n",
       "        3.28480e-02,  4.08250e-02, -8.48730e-02,  5.98010e-02,\n",
       "       -6.74240e-02,  1.65310e-02, -8.45650e-02,  5.70240e-02,\n",
       "        8.32880e-02, -1.01360e-02, -4.85080e-02,  5.17570e-02,\n",
       "        4.66640e-02,  1.81020e-02, -5.23200e-02, -7.65000e-04,\n",
       "        5.36620e-02, -9.96700e-03,  8.28580e-02,  9.06800e-03,\n",
       "        5.45750e-02, -3.46600e-03, -2.33760e-02,  2.30690e-02,\n",
       "        8.85130e-02,  1.85040e-02, -3.95030e-02, -3.29800e-02,\n",
       "       -2.13900e-03,  1.00000e-05, -1.07627e-01,  7.69900e-03,\n",
       "        4.63510e-02, -3.06200e-03,  3.05000e-02,  1.13650e-01,\n",
       "        3.25360e-02, -9.73010e-02, -1.37340e-02,  9.83450e-02,\n",
       "        8.08980e-02, -6.41730e-02, -8.87400e-03, -1.44751e-01,\n",
       "        3.75850e-02,  1.32900e-02,  5.96740e-02,  6.16300e-03,\n",
       "        7.31800e-03,  5.30000e-05, -6.02920e-02, -5.91350e-02,\n",
       "        4.94970e-02, -1.14380e-02, -9.51080e-02, -4.34650e-02,\n",
       "        4.85670e-02, -4.39900e-02, -3.07740e-02,  5.09200e-03,\n",
       "       -3.22650e-02,  9.39200e-03,  1.85030e-02,  8.48570e-02,\n",
       "        1.09709e-01, -2.06620e-02,  1.76960e-02,  2.66990e-02,\n",
       "       -7.66380e-02, -1.41060e-02, -3.51550e-02,  4.69990e-02,\n",
       "       -3.72700e-03, -4.78050e-02,  4.42700e-02,  1.13140e-02,\n",
       "        3.65240e-02, -6.95050e-02, -1.48500e-02, -3.53800e-03,\n",
       "       -4.70490e-02,  2.93490e-02,  3.45210e-02, -3.21990e-02,\n",
       "        1.16497e-01, -7.76100e-02,  6.82340e-02, -1.61260e-02,\n",
       "       -6.64540e-02, -7.99140e-02, -2.07230e-02, -6.49050e-02,\n",
       "        6.95600e-02,  2.13680e-02, -4.94970e-02, -4.65990e-02,\n",
       "        6.76630e-02, -6.90350e-02,  1.18015e-01,  2.74630e-02,\n",
       "       -6.17600e-03, -3.45140e-02, -2.65150e-02,  4.03080e-02,\n",
       "        9.11130e-02, -8.05390e-02,  1.32408e-01, -7.09580e-02,\n",
       "        1.97300e-02,  3.33990e-02,  3.48900e-03, -1.59659e-01,\n",
       "       -4.23000e-03,  4.88800e-03, -5.66150e-02,  6.10210e-02,\n",
       "        2.51170e-02,  9.96130e-02,  6.38760e-02, -6.20200e-03,\n",
       "       -4.93160e-02, -2.05300e-02,  8.52200e-03, -9.41680e-02,\n",
       "       -9.45100e-03, -3.46820e-02, -2.68010e-02,  6.53830e-02,\n",
       "        4.25280e-02, -1.36880e-02,  3.50680e-02,  1.19803e-01,\n",
       "       -2.02780e-02,  1.11882e-01, -6.83360e-02,  5.02450e-02,\n",
       "       -1.23240e-01,  2.24800e-03,  1.02290e-02, -6.25400e-02,\n",
       "       -1.78370e-02, -1.15210e-01,  5.10360e-02,  2.69200e-02,\n",
       "        8.34570e-02,  6.29020e-02,  3.08300e-03, -3.71120e-02,\n",
       "       -1.54250e-02,  1.71600e-03,  1.80020e-02,  1.01100e-01,\n",
       "        1.94460e-02,  7.48390e-02,  5.99510e-02,  7.22430e-02,\n",
       "       -2.54590e-02, -3.98530e-02,  7.79500e-02,  6.01990e-02,\n",
       "       -7.06030e-02,  6.06520e-02,  2.38550e-02, -3.58580e-02,\n",
       "       -5.80430e-02, -7.51300e-02,  6.70000e-04,  8.28280e-02,\n",
       "       -1.31410e-02,  1.44692e-01, -1.05775e-01,  8.76240e-02,\n",
       "        4.70300e-02, -1.57000e-02, -4.23600e-02,  2.63370e-02,\n",
       "        1.44966e-01,  2.19220e-02,  1.20970e-02, -1.14430e-02,\n",
       "       -1.10208e-01,  9.33300e-03, -8.14230e-02, -2.71370e-02,\n",
       "        8.27000e-04,  8.53890e-02,  3.44640e-02,  3.23380e-02,\n",
       "       -1.50400e-02,  9.95400e-03, -1.17590e-02, -4.23950e-02,\n",
       "       -4.67000e-04,  5.50470e-02, -4.97100e-02, -1.04978e-01,\n",
       "        3.17420e-02,  3.70200e-02, -7.01600e-03, -4.97120e-02,\n",
       "        4.16840e-02,  1.83480e-02, -1.20100e-03,  1.92820e-02,\n",
       "        5.76300e-03,  7.49520e-02, -1.85880e-02,  1.60550e-02,\n",
       "        1.19526e-01, -1.46130e-02,  2.05980e-02,  2.73120e-02,\n",
       "        2.42520e-02, -2.40440e-02, -2.63320e-02, -6.31520e-02,\n",
       "       -9.53630e-02, -3.43350e-02, -6.25520e-02, -3.57300e-02,\n",
       "        9.11650e-02,  9.68600e-03,  2.03410e-02, -1.20040e-02,\n",
       "        1.18260e-02, -8.43010e-02,  1.11440e-02,  7.49690e-02,\n",
       "       -4.86200e-03, -1.40760e-02,  2.56920e-02, -7.73220e-02,\n",
       "       -2.29980e-02, -1.28057e-01, -4.91700e-03,  6.26280e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvectors['de']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### palabra fuera del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'bachelet' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-49ef493d85b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bachelet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/text-classification/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'bachelet' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "wordvectors['bachelet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25726, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de los vectores para el vocabulario del corpus de entrenamiento, desde el modelo word2vect preentrenado. Si no se encuentra el vector para alguna palabra (Out of Vocabulary Word), se genera uno aleatorio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = wordvectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25726, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(wordvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 173400)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1734010   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 9,451,843\n",
      "Trainable params: 9,451,843\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "                    input_dim=vocab_size, \n",
    "                    output_dim=EMBEDDING_DIM, \n",
    "                    input_length=max_sequence_length,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=True\n",
    "                )\n",
    ")\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 81s 8ms/step - loss: 1.0124 - acc: 0.4833 - val_loss: 0.9338 - val_acc: 0.5527\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 86s 9ms/step - loss: 0.7255 - acc: 0.6764 - val_loss: 0.9676 - val_acc: 0.5498\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 81s 8ms/step - loss: 0.3591 - acc: 0.8521 - val_loss: 1.0552 - val_acc: 0.5800\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 89s 9ms/step - loss: 0.1029 - acc: 0.9717 - val_loss: 1.2413 - val_acc: 0.5772\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 87s 9ms/step - loss: 0.0541 - acc: 0.9862 - val_loss: 1.3019 - val_acc: 0.5812\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0413 - acc: 0.9903 - val_loss: 1.3250 - val_acc: 0.5868\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 83s 8ms/step - loss: 0.0355 - acc: 0.9922 - val_loss: 1.6018 - val_acc: 0.5816\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 81s 8ms/step - loss: 0.0363 - acc: 0.9909 - val_loss: 2.0037 - val_acc: 0.5639\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 81s 8ms/step - loss: 0.0336 - acc: 0.9905 - val_loss: 1.8137 - val_acc: 0.5567\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 84s 8ms/step - loss: 0.0297 - acc: 0.9913 - val_loss: 1.7319 - val_acc: 0.5740\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9948\n",
      "Testing Accuracy:  0.5740\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "[model reference](https://arxiv.org/abs/1408.5882)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 576, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 189, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 59, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,997,739\n",
      "Trainable params: 7,997,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ivan/.virtualenvs/text-classification/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 100s 10ms/step - loss: 0.9740 - acc: 0.5074 - val_loss: 0.8529 - val_acc: 0.6133\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 104s 10ms/step - loss: 0.6307 - acc: 0.7389 - val_loss: 0.8785 - val_acc: 0.6121\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 105s 11ms/step - loss: 0.2129 - acc: 0.9260 - val_loss: 1.3025 - val_acc: 0.6025\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 99s 10ms/step - loss: 0.0898 - acc: 0.9729 - val_loss: 1.4620 - val_acc: 0.5868\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 102s 10ms/step - loss: 0.0474 - acc: 0.9863 - val_loss: 1.7146 - val_acc: 0.5961\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 106s 11ms/step - loss: 0.0343 - acc: 0.9900 - val_loss: 1.7391 - val_acc: 0.6009\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 99s 10ms/step - loss: 0.0257 - acc: 0.9917 - val_loss: 1.8742 - val_acc: 0.5784\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 99s 10ms/step - loss: 0.0202 - acc: 0.9927 - val_loss: 2.0472 - val_acc: 0.5993\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 106s 11ms/step - loss: 0.0154 - acc: 0.9936 - val_loss: 2.2853 - val_acc: 0.5896\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 99s 10ms/step - loss: 0.0124 - acc: 0.9939 - val_loss: 2.4817 - val_acc: 0.5920\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9950\n",
      "Testing Accuracy:  0.5920\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = KeyedVectors.load_word2vec_format('../embeddings/glove-sbwc.i25.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = glove_vectors[word]\n",
    "        glove_embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        glove_embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 576, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 189, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 63, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 59, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,997,739\n",
      "Trainable params: 7,997,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 96s 10ms/step - loss: 0.9010 - acc: 0.5820 - val_loss: 0.8143 - val_acc: 0.6427\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.5971 - acc: 0.7541 - val_loss: 0.8749 - val_acc: 0.6423\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.2478 - acc: 0.9123 - val_loss: 1.2025 - val_acc: 0.6125\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.1095 - acc: 0.9664 - val_loss: 1.3470 - val_acc: 0.5965\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.0607 - acc: 0.9827 - val_loss: 1.6458 - val_acc: 0.5997\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.0444 - acc: 0.9892 - val_loss: 1.6890 - val_acc: 0.6133\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.0286 - acc: 0.9923 - val_loss: 1.6485 - val_acc: 0.6178\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.0214 - acc: 0.9937 - val_loss: 1.7584 - val_acc: 0.6037\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 106s 11ms/step - loss: 0.0147 - acc: 0.9944 - val_loss: 1.9791 - val_acc: 0.5949\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 114s 12ms/step - loss: 0.0110 - acc: 0.9942 - val_loss: 2.3577 - val_acc: 0.6141\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9956\n",
      "Testing Accuracy:  0.6141\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (padding 'same') + Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 578, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 192, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 192, 128)          65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 64, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,997,739\n",
      "Trainable params: 7,997,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.8925 - acc: 0.5762 - val_loss: 0.8148 - val_acc: 0.6310\n",
      "Epoch 2/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.6074 - acc: 0.7481 - val_loss: 0.8917 - val_acc: 0.6423\n",
      "Epoch 3/10\n",
      "9948/9948 [==============================] - 102s 10ms/step - loss: 0.2662 - acc: 0.9013 - val_loss: 1.1624 - val_acc: 0.6093\n",
      "Epoch 4/10\n",
      "9948/9948 [==============================] - 103s 10ms/step - loss: 0.0942 - acc: 0.9720 - val_loss: 1.6131 - val_acc: 0.5928\n",
      "Epoch 5/10\n",
      "9948/9948 [==============================] - 98s 10ms/step - loss: 0.0474 - acc: 0.9894 - val_loss: 1.7369 - val_acc: 0.5957\n",
      "Epoch 6/10\n",
      "9948/9948 [==============================] - 105s 11ms/step - loss: 0.0313 - acc: 0.9913 - val_loss: 1.7517 - val_acc: 0.5884\n",
      "Epoch 7/10\n",
      "9948/9948 [==============================] - 101s 10ms/step - loss: 0.0195 - acc: 0.9936 - val_loss: 1.9872 - val_acc: 0.6218\n",
      "Epoch 8/10\n",
      "9948/9948 [==============================] - 100s 10ms/step - loss: 0.0136 - acc: 0.9932 - val_loss: 2.4672 - val_acc: 0.5965\n",
      "Epoch 9/10\n",
      "9948/9948 [==============================] - 104s 10ms/step - loss: 0.0109 - acc: 0.9936 - val_loss: 2.5091 - val_acc: 0.5973\n",
      "Epoch 10/10\n",
      "9948/9948 [==============================] - 102s 10ms/step - loss: 0.0149 - acc: 0.9917 - val_loss: 2.3621 - val_acc: 0.6089\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,878,503\n",
      "Trainable params: 7,878,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 159s 16ms/step - loss: 1.0528 - acc: 0.4258 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 169s 17ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0507 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 171s 17ms/step - loss: 1.0508 - acc: 0.4344 - val_loss: 1.0495 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 164s 17ms/step - loss: 1.0516 - acc: 0.4344 - val_loss: 1.0496 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 155s 16ms/step - loss: 1.0503 - acc: 0.4344 - val_loss: 1.0508 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4344\n",
      "Testing Accuracy:  0.4469\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,878,503\n",
      "Trainable params: 7,878,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length, weights=[glove_embedding_matrix], trainable=True),\n",
    "    layers.LSTM(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 159s 16ms/step - loss: 1.0539 - acc: 0.4331 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0503 - acc: 0.4316 - val_loss: 1.0498 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 165s 17ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0522 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 168s 17ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0489 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 173s 17ms/step - loss: 1.0508 - acc: 0.4344 - val_loss: 1.0492 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4344\n",
      "Testing Accuracy:  0.4469\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train_padded_sequences, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test_padded_sequences, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 100)               120300    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,838,403\n",
      "Trainable params: 7,838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_sequence_length),\n",
    "    layers.GRU(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 130s 13ms/step - loss: 1.0530 - acc: 0.4282 - val_loss: 1.0500 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 132s 13ms/step - loss: 1.0518 - acc: 0.4289 - val_loss: 1.0511 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 126s 13ms/step - loss: 1.0506 - acc: 0.4306 - val_loss: 1.0508 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 133s 13ms/step - loss: 1.0505 - acc: 0.4344 - val_loss: 1.0515 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 130s 13ms/step - loss: 1.0509 - acc: 0.4344 - val_loss: 1.0520 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU with pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 578, 300)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 100)               120300    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 7,838,403\n",
      "Trainable params: 7,838,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.SpatialDropout1D(0.2),\n",
    "    layers.GRU(100),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0513 - acc: 0.4335 - val_loss: 1.0488 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "9948/9948 [==============================] - 156s 16ms/step - loss: 1.0521 - acc: 0.4344 - val_loss: 1.0512 - val_acc: 0.4469\n",
      "Epoch 3/5\n",
      "9948/9948 [==============================] - 153s 15ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0586 - val_acc: 0.4469\n",
      "Epoch 4/5\n",
      "9948/9948 [==============================] - 169s 17ms/step - loss: 1.0506 - acc: 0.4344 - val_loss: 1.0507 - val_acc: 0.4469\n",
      "Epoch 5/5\n",
      "9948/9948 [==============================] - 160s 16ms/step - loss: 1.0504 - acc: 0.4344 - val_loss: 1.0503 - val_acc: 0.4469\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN +  LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 578, 300)          7717800   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 574, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 143, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 7,837,017\n",
      "Trainable params: 7,837,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=max_sequence_length,\n",
    "        weights=[glove_embedding_matrix], \n",
    "        trainable=True\n",
    "    ),\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=4),\n",
    "    layers.LSTM(50),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9948 samples, validate on 2488 samples\n",
      "Epoch 1/5\n",
      "9948/9948 [==============================] - 105s 11ms/step - loss: 1.0539 - acc: 0.4256 - val_loss: 1.0503 - val_acc: 0.4469\n",
      "Epoch 2/5\n",
      "4832/9948 [=============>................] - ETA: 51s - loss: 1.0467 - acc: 0.4487"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_padded_sequences, y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test_padded_sequences, y_test),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "288.85px",
    "left": "205px",
    "right": "361px",
    "top": "177px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
